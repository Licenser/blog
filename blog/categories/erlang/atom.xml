<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Erlang | Lice!]]></title>
  <link href="http://blog.licenser.net/blog/categories/erlang/atom.xml" rel="self"/>
  <link href="http://blog.licenser.net/"/>
  <updated>2014-11-28T19:24:06+01:00</updated>
  <id>http://blog.licenser.net/</id>
  <author>
    <name><![CDATA[Heinz N. 'Licenser' Gies]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Asynchronously GCed or Set]]></title>
    <link href="http://blog.licenser.net/blog/2013/06/13/a-asynchronous-gced-or-set/"/>
    <updated>2013-06-13T11:09:00+02:00</updated>
    <id>http://blog.licenser.net/blog/2013/06/13/a-asynchronous-gced-or-set</id>
    <content type="html"><![CDATA[<p>Following the <a href="/blog/2013/06/11/asyncronous-garbage-collection-with-crdts/">article about Asynchronous garbage collection with CRDTs</a> I experimented with <a href="https://github.com/Licenser/ecrdt/blob/master/src/vorsetg.erl">implementing the concept</a>. The OR Set is a very nice data structure for this  since it&rsquo;s rather simple and so is it&rsquo;s garbage!</p>

<p>To garbage collect the <a href="https://github.com/Licenser/ecrdt/blob/master/src/vorset.erl">OR Set</a> we do the following, we take some of the elements of the remove set, and delete them from both the add and the remove set &ndash; this way we save the space for them and generate a new baseline.</p>

<p>First step was to implement the data structure described to hold the collectable items, I call it a <a href="https://github.com/Licenser/ecrdt/blob/master/src/rot.erl">ROT</a> (Roughly Ordered Tree) it&rsquo;s a nice name for garbage related stuff ;) and it is treeish and mostly ordered.</p>

<p>The interface of the ROT is rather simple, Elements must be time tagged, in the form {Time, Element}. Where time must not be a clock, as long as the Erlang comparison operations work on it to give an order. Then it allows asking for full buckets, and removing buckets based on their hash value and newest message timestamp.</p>

<p>While the elements in a the OR set area already tagged with a timestamp, this timestamp records addition, not deletion so it would be misleading to use them since the ROT would think the remove happened when actually the addition happened and this would violate the rule that no event can travel back behind T<sub>100</sub>. As a result we&rsquo;ll have to double timestamp the removes &ndash; as in add a second when when it was removed.</p>

<p>So since the ROT has a very similar interface to a G Set (which implemented the remove set before) the change is trivial. <strong>Remove</strong>, <strong>GC</strong> and the <strong>merge</strong> function are more interesting.</p>

<h3>remove</h3>

<p>```erlang
remove(Id, Element, ORSet = #vorsetg{removes = Removes}) &ndash;></p>

<pre><code>CurrentExisting = [Elem || Elem = {_, E1} &lt;- raw_value(ORSet),
                           E1 =:= Element],
Removes1 = lists:foldl(fun(R, Rs) -&gt;
                               rot:add({Id, R}, Rs)
                       end, Removes, CurrentExisting),
ORSet#vorsetg{removes = Removes1}.
</code></pre>

<p>```</p>

<p><code>Id</code> defaults to a the current time in nanoseconds since it&rsquo;s precise enough for most cases, but can be given any value that provides timed order. Line 2 and 3 collect all observed and not yet removed instances of the element to delete, we then fold over those instances and add each of them to the ROT.</p>

<h3>GC</h3>

<p>```erlang
gc(HashID,
   #vorsetg{</p>

<pre><code>  adds = Adds,
  removes = Removes,
  gced = GCed}) -&gt;
{Values, Removes1} = rot:remove(HashID, Removes),
Values1 = [V || {_, V} &lt;- Values],
Values2 = ordsets:from_list(Values1),
#vorsetg{adds = ordsets:subtract(Adds, Values2),
         gced = ordsets:add_element(HashID, GCed),
         removes = Removes1}.
</code></pre>

<p>```</p>

<p>To GC the set we take the <code>HashID</code>, this is what the rot returns when it reports full buckets, and in line 6 remove it from the ROT. Thankfully the ROT will return the content of the deleted bucket, this comes in very handy, since in the process of garbage collecting the bucket we also need to remove the items once and for all from the add list as seen in line 9. We then record the GC action in line 10 to make sure it will applied during a merge.</p>

<p>Please note that currently this set, even so it is garbage collected still grows without bounds since the GC actions themselves are not (yet) garbage collected, this will be added in a later iteration.</p>

<h3>merge</h3>

<p>```erlang
merge(ROTA = #vorsetg{gced = GCedA},</p>

<pre><code>  ROTB = #vorsetg{gced = GCedB}) -&gt;
#vorsetg{
   adds = AddsA,
   gced = GCed,
   removes = RemovesA}
    = lists:foldl(fun gc/2, ROTA, GCedB),
#vorsetg{
   adds = AddsB,
   removes = RemovesB}
    = lists:foldl(fun gc/2, ROTB, GCedA),
ROT1 = rot:merge(RemovesA, RemovesB),
#vorsetg{adds = ordsets:union(AddsA, AddsB),
         gced = GCed,
         removes = ROT1}.
</code></pre>

<p>```</p>

<p>Merging gets a bit more complicated due to the fact that we now have to take into account that values might be garbage collected in one set but not in the other. While merging them would do no harm it would recreate the garbage which isn&rsquo;t too nice. So what we do is applying the recorded GC actions to both sets first as seen in line 3 to 11 and then merge the remove values (line 12) finally the add values (line 13).</p>

<h2>Results</h2>

<p>I set up some proper tests for the implementation, comparing the GCed OR Set (bucket size 10) with a normal OR Set, running 1000 iterations with a set of 1000 instructions composed of 70% adds and removes, 20% merges and 10% GC events. T<sub>100</sub> is a sliding time from the allowed collection of events older then the last merge.</p>

<p>Each stored element had the size of between 500 and 600 bytes (so there were 100 possible elements). A remove will always remove the stalest element, since they are added in random order this equals a random remove.</p>

<p>The operations are carried out of replicas copies of the set where add, and remove have a equal chance to be either happening just on copy A, or just on copy B, or on both replicas at the some time. GC operations are always carried out on both replicas but it should be noted that the GC operation does not include a merge operation so can be considered asynchronous.</p>

<p>All operations but the GC operation are executed exactly the same on the GCed OR set and the not GCed or Set in the same order and same spread.</p>

<p>At the end a final merge was performed and the resulting values compared for each iteration, no additional GC action takes place at the end.</p>

<p>Measured were both the space reduction per GC run and the final difference of size. Per GC run about 15% space was reclaimed and at the end the GCed set had a total space consumption of around 26% of the normal OR Set in average, 6% in the best and 143% in the worst case.</p>

<p><code>
src/vorsetg.erl:389:&lt;0.135.0&gt;: [Size] Cnt: 1000,   Avg: 0.261,  Min: 0.062, Max: 1.507
src/vorsetg.erl:389:&lt;0.135.0&gt;: [ RS ] Cnt: 49221,  Avg: 0.866,  Min: 0.064, Max: 1.0
src/vorsetg.erl:389:&lt;0.135.0&gt;: [ GC ] Cnt: 49221,  Avg: 55.870, Min: 0,     Max: 6483
src/vorsetg.erl:389:&lt;0.135.0&gt;: [ MG ] Cnt: 98357,  Avg: 58.110, Min: 0,     Max: 6596
src/vorsetg.erl:389:&lt;0.135.0&gt;: [ OP ] Cnt: 344708, Avg: 38.539, Min: 0,     Max: 6916</code>
```</p>

<p>The numbers are from a test run, for readability truncated manually after 3 digest and aligned to be nicer readable. <strong>Size</strong> is total size at the end of the iteration, <strong>RS</strong> is the space reduction per GC run. <strong>GC</strong>, <strong>MG</strong> and <strong>OP</strong> are the time used for garbage collection, merging and other operations respectively, the numbers are per execution and measured microseconds. Time measurements also include noise that from additional operations required for the test and should not be seen as a useful benchmark!</p>

<h2>Conclusion</h2>

<p>The GC method described seems to work, and not even too badly, in the course of experimenting with values it showed that the conserved space is heavily dependant on the environment like the bucket size chosen, the size of the elements, the add/remove ratio and the ratio on which merges happen.</p>

<p>The OR Set it was compared with was not optimised at all, but thanks to it&rsquo;s simplicity a rather good candidate, the gains on already optimised sets will likely be lower. (run with a <a href="https://github.com/Licenser/ecrdt/blob/master/src/vorset2.erl">optimised OR Set</a> gave only 1 54% reduction in space instead of a 74% one with a normal OR Set).</p>

<p>The downside is that garbage collection takes time, so does merging, so a structure like this is over all slower then a not garbage collected version</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Asynchronous Garbage Collection With CRDTs]]></title>
    <link href="http://blog.licenser.net/blog/2013/06/11/asyncronous-garbage-collection-with-crdts/"/>
    <updated>2013-06-11T15:45:00+02:00</updated>
    <id>http://blog.licenser.net/blog/2013/06/11/asyncronous-garbage-collection-with-crdts</id>
    <content type="html"><![CDATA[<p>So CRDTs are very very nice data structures awesome for eventual consistent applications like riak, or the components of <a href="http://project-fifo.net">Project-FiFo</a>. So they have one big drawback, most of them collect garbage, and over time that can sum up to a lot making them pretty unpractical in many cases. Collecting this garbage is a bit tricky, since usually it means synchronising the data &ndash; which going back to the eventual consistent stuff prevents either A or P.</p>

<p>I want to outline some thoughts here how one could deal with this issue. As usual the idea here isn&rsquo;t without tradeoffs, it does impose certain constrains on the systems behaviour and does not fit every behaviour in exchange of allowing garbage to be disposed of without the need of synchronisation. Now then, lets dive right in.</p>

<h2>What&rsquo;s that trash?</h2>

<p>We start with understanding what the garbage is that sums up. To allow CRDTs to work the way we do, they need to store some kind of history or legend of how the current state (version/value) of the CRDT came to existence.</p>

<p>If we look at a OR Set for example the history of this set is stored by recording all elements ever added along with all elements ever deleted &ndash; elements are tagged to be unique too so adding 5, removing 5 and adding 5 again and removing that again, leaves not a data structure with 0 elements but one with 4. That said there are ways to optimise the OR Set bot lets ignore this for the sake of the example. We can&rsquo;t just store an empty list since we need to make sure that when another copy of the same set can recreate the steps even if it just missed one of the events.</p>

<p>Actually we could, if we would synchronise all copies, say hey ¯from now on you all agree that the new <strong>baseline</strong> (this is bold since it will come up a few more times) is an empty set from now on. And  doing that we would have garbage collected the OR Set, disposed of data that isn&rsquo;t directly relevant to the current state any more.</p>

<p>If we don&rsquo;t guarantee that all objects are garbage collected to the same state, we face a real issue, since the new baseline will cause quite some trouble since the partially applied effects will just be applied again and possibly cause them to be doubly applied. <strong>Or in short, partially applied GCing will cause the CRDT to stop functioning.</strong></p>

<h2>Things get old.</h2>

<p>Looking at the data that gathers and how it is distributed there is one observation to be made: the older a change in state is the more likely it is to be present in all replicas. It makes sense, with eventual consistency we say &lsquo;eventually&rsquo; our data will be the same everywhere, and the chances of &lsquo;eventual&rsquo; are growing the older the change is since it will get more chance to replicate. (mechanisms similar to riak&rsquo;s AAE greatly help here).</p>

<p><img src="/images/posts/2013-06-11-asynchronous-garbage-collection-with-crdts1.png" alt="state distribution" /></p>

<p>So generally there is a T<sub>100</sub> from which point on older data is shared between all instances and by that no longer relevant if we could just garbage collect it. But we don&rsquo;t want synchronous operations, nor do we want partial garbage collection (since that rally would suck).</p>

<p>Back to state, we know which ones we want to garbage collect, lets say we record not only the state change but a timestamp, a simple non monotonic system timestamp, it&rsquo;s cheap to get. Keep in  mind T<sub>100</sub> is well in the past, so if the precision of the times taps is good enough to guarantee that a event at T<sub>0</sub> can not travel back behind T<sub>100</sub>, it&rsquo;s OK if order between T<sub>0</sub> and T<sub>99</sub> changes all the time, we don&rsquo;t really care about that so lets store the state data in a way that helps us with this:</p>

<p>T<sub>0</sub> [S<sub>0</sub>,S<sub>1</sub>, &hellip;, S<sub>n</sub>] T<sub>100</sub> [S<sub>n+1</sub>, &hellip;, S<sub>n+m</sub>]</p>

<h2>A trash bin</h2>

<p>But since it would really suck (I know I&rsquo;m repeating myself) if we partially GC the data we want to be sure that we agree, so would could go and ask all the replicas for their old data (older then T<sub>100</sub>). Yet this approach has a problem, for once T<sub>100</sub> will shift in the time we check, then this might be more data to move then we care for.</p>

<p>So lets use a trash bin, or multiple once order our data in them so you&rsquo;ve some groups of old messages, bunched together which can be agreed on, no matter on the time moving and they are smaller portions. Something like this</p>

<p>&hellip; T<sub>100</sub> [S<sub>n+1</sub>, &hellip;, S<sub>n+100</sub>] [S<sub>n+101</sub>, &hellip;, S<sub>n+200</sub>]&hellip;</p>

<p>So we just have to agree on some bucket to garbage collect, since so if there is another half full bucket now since T<sub>100</sub> has moved since the agreement we don&rsquo;t really care about that. Thanks to the fact that operations are commutative we also can garbage collect in a non direct order, so it&rsquo;s not a biggie if we take just one bucket and not the oldest one.</p>

<p>We&rsquo;re still left with transmitting (in this example) 100 elements to delete and haven&rsquo;t solve the problem of partial garbage collection, but at least we&rsquo;re a good step closer, we&rsquo;ve put the garbage in bins now that are much easier to handle then just on a huge pile.</p>

<h2>A garbage compactor</h2>

<p>Lets tackle the last two issues we do a little trick, instead of sending out the entire bucket we compress it, create a hash of it and send this back and forth, so instead of:</p>

<p>[S<sub>n+1</sub>, &hellip;, S<sub>n+100</sub>]</p>

<p>We tag this bucket with a hash (over it&rsquo;s content) and the newest timestamp of the first element. Since it&rsquo;s older then T<sub>100</sub> we do not need to worry of it changing and recreating the hash, and we get something like this:</p>

<p>(hash, T<sub>S<sub>n+1</sub></sub>)[S<sub>n+1</sub>, &hellip;, S<sub>n+100</sub>]</p>

<p>To agree on buckets to collect and to give the collect order we just need to send the hash and timestamp and an identifier, this is pretty little data to send forth and back. This solves the send much data problem, curiously it also helps a lot with the partial garbage collection status.</p>

<h2>A schedule for garbage collection</h2>

<p>With only the buckets tag identifying it we can solve the partial collection issue, we just treat garbage collection as just another event, storing it and replaying it if it wasn&rsquo;t present in a old replica. So we gradually progress the baseline of a replica towards the common baseline somewhat like this:</p>

<p><img src="/images/posts/2013-06-11-asyncronous-garbage-collection-with-crdts2.png" alt="gc graph" /></p>

<p>Ideally we store the GC operations in a own list and since we can easier apply it then and guarantee that the GC events are synchronised and applied before other events.</p>

<p>That&rsquo;s it, and should be a somewhat working implementation of asynchronous garbage collection for CRTDs. But it&rsquo;s not perfect so lets take a look at the downsides before we end this.</p>

<h2>Lets be honest, it still has a downside</h2>

<p>This concept of GCing does not come for free, the data structure required isn&rsquo;t entirely trivial so it will add overhead, even so the current <a href="https://github.com/Licenser/ecrdt/blob/master/src/rot.erl">implementation</a> is pretty cheap when adding the events in right order, wrong order will cause additional overhead because it might cause elements to shift around in the structure.</p>

<p>It requires events to be timestamped, even so there is no requirement for absolute order, this adds a constraint to messages and events that wasn&rsquo;t there before. Also this is additional work and space that is consumed.</p>

<p>We need to define a T<sub>100</sub> for the system and guarantee it, and find a balance of choosing a big enough T<sub>100</sub> to ensure it&rsquo;s correctness while keeping it small enough to not keep a huge tail of non garbage collected events. That said this can be mitigated slightly by using a dynamic T<sub>100</sub> for example put record when a object was last written to all primary nodes.</p>

<p>If T<sub>100</sub> isn&rsquo;t chooses correctly it might end up getting really messy! if a elements slips by T<sub>100</sub> that wasn&rsquo;t there it could mean that the garbage collection is broken for quite some while or worst state gets inconsistent.</p>

<p>Bucket size is another matter, it needs to be chosen carefully to be big enough to not spam the system but small enough to not take ages to fill, a event passing T<sub>100</sub> but not filling the bucket isn&rsquo;t doing much good.</p>

<p>This is just a crazy idea. I haven&rsquo;t tried this, implemented it or have a formal prove, it is based on common sense and my understanding on matters so it might just explode ;)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Writing Your First Riak Test Test (Yes I Know There Are Two Tests There)]]></title>
    <link href="http://blog.licenser.net/blog/2013/04/09/writing-your-first-riak-test-test-yes-i-know-there-are-two-tests-there/"/>
    <updated>2013-04-09T23:33:00+02:00</updated>
    <id>http://blog.licenser.net/blog/2013/04/09/writing-your-first-riak-test-test-yes-i-know-there-are-two-tests-there</id>
    <content type="html"><![CDATA[<p>As promised in a <a href="/blog/2013/03/31/getting-started-with-riak-test-and-riak-core/">previous post</a> I&rsquo;ll talk a bit about writing tests for <code>riak_test</code>. To start with the obvious it&rsquo;s pretty simple and pretty awesome. <code>riak_test</code> gives you the tools you&rsquo;ve dreamed of when testing distributed <code>riak_core</code> applications:</p>

<ul>
<li>a backchannel to communicate and execute commands on the nodes.</li>
<li>a nice and way to bring up and tear down the test environment.</li>
<li>helper functions to deal with the <code>riak_core</code> cluster.</li>
<li>something called <code>intercepts</code> that allow you to mock certain behaviours in the cluster.</li>
<li>all the power of Erlang.</li>
</ul>


<h2>How a test works</h2>

<p>Tests have a very simple structre they pretty much contain a single function: <code>confirm/0</code></p>

<p>This function gets called when the test is executed and should return <code>pass</code> when everything works well or throw an exception when not. The actual test are simple unit asserts you use.</p>

<p>That in itself is not really overly exciting and those of you with a short attention span might start to thing &lsquo;boooooooring&rsquo; so lets look at the exciting part.</p>

<h2>Starting your applicatio</h2>

<p><code>riak_test</code> offers a way to start instances of your application and communicate with them, the common pattern is to start one (or more) nodes as first part of the script and check of they are up and running. That could look something like this:</p>

<p>```erlang
confirm() &ndash;></p>

<pre><code>[Node] = rt:deploy_nodes(1),
?assertEqual(ok, rt:wait_until_nodes_ready([Node])),
pass.
</code></pre>

<p>```</p>

<p>This is a minimal test it sets up one instance of our application and waits for it to be ready.</p>

<p><code>rt:deploy_nodes(1)</code> will deploy one node, the id of the node (a atom with that identifies it to erlang) will be sotred in <code>Node</code>, you can deploy more nodes by increasing the number in <code>rt:deploy_nodes/1</code>.</p>

<p><code>?assertEqual(ok, rt:wait_until_nodes_ready([Node]))</code> will make the test wait for the nodes to be ready, ready here means that all ring services we defined in the config are provided.</p>

<p>The node will be running in its own Erlang VM and have the test suite connected as a hidden node. This is the first thing that is truly interesting, since the connection will allow us to run rpc calls on the node this is the first thing that is truly fun.</p>

<h2>An official channel</h2>

<p>Now we&rsquo;ve a basic test running have our nodes to be started up and the test waiting until all is started and happy.</p>

<p>So chances are that aside of this back channel communication the node provides some kind of API, and we want to be able to connect to this API to run our tests. Un my case it&rsquo;s a simple TCP port that announces itself over mdns, we could simply listen to the broadcast and use the information it provides to talk to the node. This would work, as long as we&rsquo;ve a single node, the moment we&rsquo;ve to we would never know which node we&rsquo;re talking to and that would make testing hard.</p>

<p>So backchannel to the rescue! We&rsquo;ll just get the nodes configuration from the host, for my application I store this kind of information in the application configuration so I&rsquo;ve made a function that given a <code>Node</code> returns <code>IP</code> and <code>Port</code> for it to talk to plus one to send data:</p>

<p>```erlang
node_endpoing(Node) &ndash;></p>

<pre><code>{ok, IP} = rpc:call(Node, application, get_env, [mdns_server_lib, ip]),
{ok, Port} = rpc:call(Node, application, get_env, [mdns_server_lib, port]),
{IP, Port}.
</code></pre>

<p>call(Node, Msg) &ndash;></p>

<pre><code>{IP, Port} = node_endpoing(Node),
lager:debug("~s:~p &lt;- ~p", [IP, Port, Msg]),
{ok, Socket} = gen_tcp:connect(IP, Port, [binary, {active,false}, {packet,4}], 100),
ok = gen_tcp:send(Socket, term_to_binary(Msg)),
{ok, Repl} = gen_tcp:recv(Socket, 0),
{reply, Res} = binary_to_term(Repl),
lager:debug("~s:~p -&gt; ~p", [IP, Port, Res]),
gen_tcp:close(Socket),
Res.
</code></pre>

<p>```</p>

<p>There is some <code>gen_tcp</code> stuff in there but lets just ignore it for now it&rsquo;s detail not important, the first function is the more interesting one it uses the <code>rpc</code> module to execute the commands on the node we just started which is quite awesome.</p>

<p>As a note I&rsquo;ve put those functions into <code>rt_&lt;applicatin name&gt;</code> so for example rt_sniffle.</p>

<h2>Testing the API</h2>

<p>With <code>call/2</code> we&rsquo;ve now a way to send data directly to the node over the official API channel so lets add to our test. So lets get back to our <code>confirm/0</code> and add some real tests:</p>

<p>```erlang
confirm() &ndash;></p>

<pre><code>[Node] = rt:deploy_nodes(1),
?assertEqual(ok, rt:wait_until_nodes_ready([Node])),
?assertEqual({ok,[]},
             rt_sniffle:call(Node, {vm, list})),
?assertEqual(ok,
             rt_sniffle:call(Node, {vm, register, &lt;&lt;"vmid"&gt;&gt;, &lt;&lt;"hypervisor"&gt;&gt;})),
?assertEqual({ok,[&lt;&lt;"vmid"&gt;&gt;]},
             rt_sniffle:call(Node, {vm, list})),                 
pass.
</code></pre>

<p>```</p>

<p>It&rsquo;s as easy as this, this test will</p>

<ul>
<li>List the registered VM&rsquo;s and expect none to be there.</li>
<li>Create a new VM and expect a ok result.</li>
<li>List the registered vm&rsquo;s and expect the one we just registered to be present.</li>
</ul>


<p>And that&rsquo;s it for basic testing with <code>riak_test</code> I&rsquo;ll follow up on this with an article over intercepts since they add another cool feature to <code>riak_test</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Riak_test and Riak_core]]></title>
    <link href="http://blog.licenser.net/blog/2013/03/31/getting-started-with-riak-test-and-riak-core/"/>
    <updated>2013-03-31T07:30:00+02:00</updated>
    <id>http://blog.licenser.net/blog/2013/03/31/getting-started-with-riak-test-and-riak-core</id>
    <content type="html"><![CDATA[<p>If you don&rsquo;t know what <code>riak_core</code> is, or don&rsquo;t have a <code>riak_core</code> based application you&rsquo;ll probably not take too much practical use of this posts you might want to start with <a href="https://github.com/rzezeski/try-try-try">Ryan Zezeski&rsquo;s &ldquo;working&rdquo; blog try try try</a> and the <a href="https://github.com/basho/rebar_riak_core">rebar plugin</a>.</p>

<p>That said if you have a <code>riak_core</code> app this posts should get you started on how to test it with <code>riak_test</code>. We&rsquo;ll not go through the topic of how to write the tests itself, this might come in a later post also the <code>riak_kv</code> tests are a good point to start.</p>

<p>Please note that the approach described here is what I choose to do, it might not be best practice of the best way for you to do things. I also will link to <a href="https://github.com/Licenser/riak_test">my fork</a> of <code>riak_test</code> instead of the <a href="https://github.com/basho/riak_test">official one</a> since it includes some modifications required for testing apps other then <code>riak_kv</code> I hope those modifications will be merged back at some point but for now I want to get it ironed out a bit more before making a pull request.</p>

<h2>What is riak_test?</h2>

<p>So before we start a few words to <code>riak_test</code>. <code>riak_test</code> is a pretty nice framework for testing distributed applications, it is, just as about all other <code>riak_</code> stuff created by <a href="http://basho.com">Basho</a> and it is pretty darn awesome.</p>

<p>At it&rsquo;s current state it is very focused on testing <code>riak_kv</code> (or <code>riak</code> as in the database) but from a first glance a lot of functionality is very universal and after all <code>riak_core</code> is also build on top of <code>riak_core</code>, so modifying it to run with other <code>riak_core</code> based apps is pretty easy.</p>

<h2>The setup</h2>

<p>Since I will be testing multiple <code>riak_core</code> apps and not just one I decided to go the following path: Have the entire setup in a git repository, then have one branch for general fixes/changed to <code>riak_core</code> then have one branch for each application I want to test that is based on the general branch so common changes can easily be merged so it will look like this:</p>

<p><code>``
---riak_test--------- (bashos master tree)
  </code>&mdash;-riak_core&mdash;&mdash;&ndash; (modifications to make riak_test work with core apps)</p>

<pre><code>` `  `---sniffle- (tests for sniffle)
 ` `---snarl----- (tests for snarl)
  `---howl------- (tests for howl)
</code></pre>

<p>```</p>

<p>We&rsquo;ll go over this and setting up tests for the <a href="https://github.com/project-fifo/howl">howl application</a> it&rsquo;s rather small and simple and it&rsquo;s easier to follow along with something real instead of a made up situation.</p>

<h2>Getting started</h2>

<p>Step one of getting started is to get a clone from the <code>riak_test</code> repository, that&rsquo;s pretty simple (alter the path if you decided to fork):</p>

<p><code>
cd ~/Projects
git clone https://github.com/Licenser/riak_test.git
cd riak_test
</code></p>

<p>Now we branch of to have a place to get our howl application but first we need to checkout the riak_core branch to make sure we get the changes included in it:</p>

<p><code>
git checkout riak_core
git branch howl
git checkout howl
</code></p>

<p>Okay that&rsquo;s it for the basic setup not that bad so far is it?</p>

<h2>Configuration</h2>

<p>Next thing we need to do is creating a configuration, at this point we assume you don&rsquo;t have any yet so we&rsquo;ll start from scratch, if you add more then one application later on you can just add them to an existing configuration.</p>

<p><code>riak_test</code> looks for the configuration file <code>~/.riak_test.config</code> and reads all the data from there so we&rsquo;ll first need to copy the sample config there:</p>

<p><code>
cp riak_test.config.sample ~/.riak_test.config
</code></p>

<p>Next step is to open it in your favourite editor, you&rsquo;ll recognise it&rsquo;s a good old Erlang config file with tuples to group sections. We&rsquo;ll be ignoring the <code>default</code> section for now, if you&rsquo;re interested in it the documentation is quite good!</p>

<p>So lets go down to where it reads:</p>

<p><code>erlang
%% ===============================================================
%%  Project-specific configurations
%% ===============================================================
</code></p>

<p>Here is where the fun starts, you&rsquo;ll see a tuple starting with <code>{rtdev,</code> &ndash; note this <code>rtdev</code> has nothing whatsoever to do with the <code>rtdev</code> that is in the <code>default</code> section as <code>{rt_harness, rtdev}</code>. The <code>rtdev</code> in the project part is just the name of the project, since your project is named <code>howl</code> not <code>rtdev</code> we&rsquo;ll go and change that first.</p>

<p><code>erlang
{rtdev, [
</code></p>

<p>Now we can go to set up some variables first up the project name and executables, the name itself is just for information (or if you use <code>giddyup</code>) the executables are how your application is started, since our application is named <code>howl</code> it&rsquo;s started with the command <code>howl</code> and the admin command for it is <code>howl-admin</code>.</p>

<p>```erlang</p>

<pre><code>%% The name of the project/product, used when fetching the test
%% suite and reporting.
{rt_project, "howl"},

{rc_executable, "howl"},
{rc_admin, "howl-admin"},
</code></pre>

<p>```</p>

<p>With that done come the services, those are the buggers you register in your _app.erl file, lets have a look at the howl_app.erl:</p>

<p>```erlang
%&hellip;</p>

<pre><code>        ok = riak_core_node_watcher:service_up(howl, self()),
</code></pre>

<p>%&hellip;
```</p>

<p>So we only have one service here we need to watch out for, named, you might guess … right <code>howl</code> that makes the list rather short:</p>

<p>```erlang</p>

<pre><code>{rc_services, [howl]},
</code></pre>

<p>```</p>

<p>Now the cookie, it&rsquo;s a bit hidden in the code that you need to set it but you do, you will need this later so remember it! Since I am bad at remembering things I named it … <code>howl</code> … again.</p>

<p>```erlang</p>

<pre><code>{rt_cookie, howl},
</code></pre>

<p>```</p>

<p>Now comes the setup of paths, for this we&rsquo;ve to decide where we want to put our data later on, I&rsquo;ve put all my <code>riak_test</code> things in <code>/Users/heinz/rt/...</code> so we&rsquo;ll follow with this. Also note that my development process works on three branches:</p>

<ul>
<li><code>test</code> &ndash; the most unstable branch.</li>
<li><code>dev</code> &ndash; here things go that should work.</li>
<li><code>master</code> &ndash; only full releases go in here.</li>
</ul>


<p>This setup might not work for you at all, but since it are only path names it should be easy enough to adept the.</p>

<p>Note that by default <code>riak_test</code> will run tests on the <code>current</code> environment</p>

<p>```erlang</p>

<pre><code>%% Paths to the locations of various versions of the project. This
%% is only valid for the `rtdev' harness.
{rtdev_path, [
              %% This is the root of the built `rtdev' repository,
              %% used for manipulating the repo with git. All
              %% versions should be inside this directory.
              {root, "/Users/heinz/rt/howl"},

              %% The path to the `current' version, which is used
              %% exclusively except during upgrade tests.
              {current, "/Users/heinz/rt/howl/howl-test"},

              %% The path to the most immediately previous version
              %% of the project, which is used when doing upgrade
              %% tests.
              {previous, "/Users/heinz/rt/howl/howl-dev"},

              %% The path to the version before `previous', which
              %% is used when doing upgrade tests.
              {legacy, "/Users/heinz/rt/howl/howl-stable"}
             ]}
</code></pre>

<p>]}
```</p>

<p>And that&rsquo;s it now the config is set up and should look like this:</p>

<p>```erlang
{rtdev, [</p>

<pre><code>%% The name of the project/product, used when fetching the test
%% suite and reporting.
{rt_project, "howl"},

{rc_executable, "rhowl"},
{rc_admin, "howl-admin"},
{rc_services, [howl]},
{rt_cookie, howl},
%% Paths to the locations of various versions of the project. This
%% is only valid for the `rtdev' harness.
{rtdev_path, [
              %% This is the root of the built `rtdev' repository,
              %% used for manipulating the repo with git. All
              %% versions should be inside this directory.
              {root, "/Users/heinz/rt/howl"},

              %% The path to the `current' version, which is used
              %% exclusively except during upgrade tests.
              {current, "/Users/heinz/rt/howl/howl-test"},

              %% The path to the most immediately previous version
              %% of the project, which is used when doing upgrade
              %% tests.
              {previous, "/Users/heinz/rt/howl/howl-dev"},

              %% The path to the version before `previous', which
              %% is used when doing upgrade tests.
              {legacy, "/Users/heinz/rt/howl/howl-stable"}
             ]}
</code></pre>

<p>]}
```</p>

<h2>Setting up the application</h2>

<p>We&rsquo;ve <code>riak_test</code> ready to test now next we need to prepare howl to be tested, we&rsquo;ll only look at the <code>current</code> (aka <code>test</code>) setup since the steps for others are pretty much the same.</p>

<p>The first step is that we need the folder, so lets create it</p>

<p><code>
mkdir -p /Users/heinz/rt/raw/howl
cd /Users/heinz/rt/raw/howl
</code></p>

<p>Since howl lives with the octocat on github it&rsquo;s easy to fetch our application and checkout the test branch (remember <code>current</code> is on the test branch for me):</p>

<p><code>
git clone https://github.com/project-fifo/howl.git howl-test
cd howl-test
git checkout test
</code></p>

<p>And done, now since it&rsquo;s a <code>riak_core</code> app we should have a task called <code>stagedevrel</code> in our makefile which will basically generate three copies of <code>howl</code> for us in the folders <code>dev/dev{1,2,3}</code> and in the process take care of compiling and getting the dependencies. I prefer <code>stagedevrel</code> over the normal <code>devrel</code> since later on it will it easier to recompile code files (<code>make</code> is enough) because it links them to the right place instead of copying the.</p>

<p><code>
make stagedevrel
</code></p>

<p>Now we&rsquo;ve to do a bit of a cheating, riak_text expects the root dir to be a git repository, that is why we can&rsquo;t just put the data in there directly, so we&rsquo;ve to manually build the tree for riak core and set up as git repositor.</p>

<p>```
mkdir -p /Users/heinz/rt/howl
cd /Users/heinz/rt/howl
git init</p>

<p>cat &lt;<EOF > /Users/heinz/rt/howl/.gitignore
<em>/dev/</em>/bin
<em>/dev/</em>/erts-<em>
</em>/dev/<em>/lib
</em>/dev/*/releases
EOF
```</p>

<p>Now we need to link our devrel files and for my setup I&rsquo;ve to copy the <code>*.example</code> files of the <code>app.config</code> and <code>vm.args</code> into the right place they might be named differently for you.</p>

<p>```
export RT_BASE=/Users/heinz/rt/howl/howl-test
export RC_BASE=/Users/heinz/rt/raw/howl/howl-test
for i in 1 2 3 4
do
  mkdir -p ${RT_BASE}/dev/dev${i}/
  cd ${RT_BASE}/dev/dev${i}/
  mkdir data etc
  touch data/.gitignore
  ln -s ${RC_BASE}/dev/dev${i}/{bin,erts-*,lib,releases} .
  cp ${RC_BASE}/dev/dev${i}/etc/vm.args.example etc/vm.args
  cp ${RC_BASE}/dev/dev${i}/etc/app.config.example etc/app.config
done</p>

<p>```</p>

<p>We still need to edit the <code>vm.args</code> in <code>dev/dev{1,2,3,4}/etc/</code>  since we need to set the correct cookie &ndash; I hope you still remember yours, I told you you&rsquo;d need it (if not you can just look in the <code>~/.riak_test.config</code>)!</p>

<p>That&rsquo;s it.</p>

<h2>Running a first test</h2>

<p>In the <code>riak_core</code> branch of <code>riak_test</code> I&rsquo;ve moved all the <code>riak_kv</code> specific tests from <code>tests</code> to <code>tests_riakkv</code> so you still can look at them but I left one of them in <code>tests</code>, namely the basic command test &ndash; it will check if your applications command (<code>howl</code> in our case) is well behaved.</p>

<p>We&rsquo;ll want to run it to see if <code>howl</code> is a good boy and does well to do so we&rsquo;ll need to get back into the <code>riak_test</code> folder and run the <code>riak_test</code> command:</p>

<p><code>
cd ~/Projects/riak_test
./riak_test -t tests/* -c howl -v -b none
</code></p>

<p>I&rsquo;d like to explain this a bit, the arguments have the following meaning:</p>

<ul>
<li><code>-t tests/*</code> &ndash; we&rsquo;ll be running all tests in the folder <code>tests/</code>.</li>
<li><code>-c howl</code> &ndash; our application we want to test is named howl, this is the first element of the tuple we put in our config file when you remember.</li>
<li><code>-v</code> &ndash; This just turns on verbose output.</li>
<li><code>-b none</code> &ndash; This is still a relict from the <code>riak_kv</code> roots, it means which backend to test with, since we don&rsquo;t have backends at all we&rsquo;ll just pass none which means riak_test will happily ignore it.</li>
</ul>


<p>That&rsquo;s it! Now go and test all the things!</p>

<p>This is the first part of a series that goes on <a href="/blog/2013/04/09/writing-your-first-riak-test-test-yes-i-know-there-are-two-tests-there/">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Plugins With Erlang]]></title>
    <link href="http://blog.licenser.net/blog/2013/03/19/plugins-with-erlang/"/>
    <updated>2013-03-19T16:45:00+01:00</updated>
    <id>http://blog.licenser.net/blog/2013/03/19/plugins-with-erlang</id>
    <content type="html"><![CDATA[<h2>Preamble</h2>

<p>Lets start with this, Erlang releases are super useful they are one of the features I like most about Erlang &ndash; you get out an entirely self contained package you can deploy and forget, no library trouble, no wrong version of the VM no trouble at all.</p>

<p>BUT (this had to come didn&rsquo;t it) sometimes they are limiting and kind of inflexible, adding a tiny little feature means rolling out a new release, with automated builds that is not so bad but &lsquo;not so bad&rsquo; isn&rsquo;t good either. And things get worst when there are different wishes.</p>

<p>A little glimpse into reality: I&rsquo;m currently working a lot on <a href="http://project-fifo.net">Project FiFo</a> and one of the issues I faced is that &ndash; surprisingly &ndash; not everyone wants things to work exactly as I do. Which was a real shock, how could anyone ever disagree with me? Well &hellip; I got over it, really I did, still solving this issue by adding one code path for every preference and making it configureable didn&rsquo;t looked like a good solution.</p>

<p>Also recently we are thinking a lot about performance metrics, and there are like a gazillion of them and if you pick two random people I think they want three different sets of metrics. Ask again after 5 minutes and their opinions changed to 7 new metrics and certainly not the old ones!</p>

<h2>Plugins</h2>

<p>The problem is a very old one, extending the software after it was shipped, possibly letting the community extend it beyond what was dreamed of in the beginning. The solution is pretty much one day younger then the problem: plugins. Meaning a way to load code into the system.</p>

<p>Sounds easy but it is a bit more complex, just having something load into the VM doesn&rsquo;t do much good when it does not get executed in the proper place &ndash; so sadly this comes with extra work for the developer to sprinkle their code with hooks and callbacks for the plugins.</p>

<p>With this I&rsquo;d like to introduce <a href="https://github.com/Licenser/eplugin">eplugin</a> it&rsquo;s a very simplistic library for exactly that task &ndash; introduce plugins in an Erlang release or application. It takes care of discovering and loading plugins, letting them register into certain calls, a little dependency management on startup and provides functions to call registered plugins. Erlang comes with great tools already to the whole thing sums up to under 400 LOC.</p>

<h2>Types of plugins</h2>

<p>I feel it&rsquo;s kind of interesting to look at the different kind of plugins that exist and how to handle the cases with <a href="https://github.com/Licenser/eplugin">eplugin</a> also a post entirely without code would look boring.</p>

<h3>informative plugins</h3>

<p>Sometimes a plugin just want to know that something happened but don&rsquo;t care about the result. <code>eplugin</code> provides the <code>call</code> (and <code>apply</code>) functions for that. A logger is a good example for this so lets have a look:</p>

<p>```erlang
%%% plugin.conf
{syslog_plugin,
  [{syslog_plugin, [{&lsquo;some:event&rsquo;, log}]}],
  []}.</p>

<p>%%% syslog_plugin.erl
-module(syslog_plugin).
-export([log/1]).</p>

<p>log(String) &ndash;>
  os:cmd(&ldquo;logger &lsquo;&rdquo; ++ String ++ &ldquo;&rsquo;&rdquo;).</p>

<p>%%% in your code
  %%&hellip;
  eplugin:call(&lsquo;some:event&rsquo;, &ldquo;logging this!&rdquo;),
  %%&hellip;
```</p>

<p>Thats pretty much it, provided you&rsquo;ve started the <code>eplugin</code> application in your code and put the plugins in the right place this will just work. You could also use this to trigger side effects, like delete all files when an error occurs to remove traces of your failure.</p>

<h3>messing around plugins</h3>

<p>This kind of plugins process some data and return a new version of this data, we have <code>fold</code> for this case. Fold since it internally uses fold to pass the data from one plugin to another. there are many applications for that one would be to replace all occurrences of &lsquo;not js&rsquo; with &lsquo;node.js&rsquo; to prevent freudian typos in your texts.</p>

<p>```erlang
%%% plugin.conf
{not_js,
  [{not_js, [{&lsquo;text:check&rsquo;, replace}]}],
  []}.</p>

<p>%%% not_js.erl
-module(not_js).
-export([replace/1]).</p>

<p>replace(String) &ndash;>
  re:replace(String, &ldquo;not js&rdquo;, &ldquo;node.js&rdquo;, [global]).
%%% in your code
  %%&hellip;
  String1 = eplugin:fold(&lsquo;text:check&rsquo;, &ldquo;I&rsquo;m writing a not js application!&rdquo;),
  %%&hellip;
```</p>

<p><code>fold</code> and <code>call</code> are the most interesting and important kind of plugins, they cover most if not all of the possible use cases of plugins so there is a special case left which I found useful to have.</p>

<h3>checking plugins</h3>

<p>Checking plugins are plugins which are supposed to decide if something is Ok or not, they are pretty much a case of <code>fold</code> that returns true or false (or actually whatever is not true). But <code>eplugin</code> solves this too, with the <code>test</code> function! An example here is authentication</p>

<p>```erlang
%%% plugin.conf
{get_out,
  [{get_out, [{&lsquo;login:allowed&rsquo;, no_really_not}]}],
  []}.</p>

<p>%%% get_out.erl
-module(get_out).
-export([no_really_not/1]).</p>

<p>no_really_not(Login) &ndash;>
  {forbidden, [&ldquo;Dear &rdquo;, Login, &ldquo; we don&rsquo;t want you here go away!&rdquo;]}.</p>

<p>%%% in your code
  %%&hellip;
  case eplugin:test(&lsquo;login:allowed&rsquo;, &ldquo;Licenser&rdquo;) of</p>

<pre><code> true -&gt;
    %%% huzza!
 Error -&gt;
    io:format("~p~n", [Error])
</code></pre>

<p>  end
  %%&hellip;
```</p>
]]></content>
  </entry>
  
</feed>
