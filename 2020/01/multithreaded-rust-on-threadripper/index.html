<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta property="og:title" content="Multithreaded rust on Threadripper"><meta property="og:description" content="I recently ran some benchmarks on a Threadripper 3960X system and the results were surprising me quite a bit. Simplified, the throughput the benchmark recorded went down, from 341 MB/s on a MBP to 136 MB/s on a Threadripper desktop. Prior I had read Daniel Lemire's notes on the sub optimal performance for simdjson on Zen 2, which is heavily used in the benchmark, but the suggested drop were a few percent not half."><meta property="og:type" content="article"><meta property="og:url" content="https://blog.licenser.net/2020/01/multithreaded-rust-on-threadripper/"><meta property="article:published_time" content="2020-01-11T00:00:00+00:00"><meta property="article:modified_time" content="2020-01-11T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Multithreaded rust on Threadripper"><meta name=twitter:description content="I recently ran some benchmarks on a Threadripper 3960X system and the results were surprising me quite a bit. Simplified, the throughput the benchmark recorded went down, from 341 MB/s on a MBP to 136 MB/s on a Threadripper desktop. Prior I had read Daniel Lemire's notes on the sub optimal performance for simdjson on Zen 2, which is heavily used in the benchmark, but the suggested drop were a few percent not half."><meta name=generator content="Hugo 0.62.2"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Multithreaded rust on Threadripper","url":"https:\/\/blog.licenser.net\/2020\/01\/multithreaded-rust-on-threadripper\/","wordCount":"1158","datePublished":"2020-01-11T00:00:00+00:00","dateModified":"2020-01-11T00:00:00+00:00","author":{"@type":"Person","name":"Heinz N. Gies"},"keywords":"rust"}</script><link rel=canonical href=https://blog.licenser.net/2020/01/multithreaded-rust-on-threadripper/><title>Multithreaded rust on Threadripper | Lice!</title><link href=https://blog.licenser.net/css/style.6da5c906cc7a8fbb93f31cd2316c5dbe3f19ac4aa6bfb066f1243045b8f6061e.css rel=stylesheet integrity="sha256-baXJBsx6j7uT8xzSMWxdvj8ZrEqmv7Bm8SQwRbj2Bh4=" crossorigin=anonymous><script defer src=https://blog.licenser.net/js/fontawesome.min.90e14c13cee52929ac33e1c21694a3cc95063a194eb22aad9f7976434e1a9125.js integrity="sha256-kOFME87lKSmsM+HCFpSjzJUGOhlOsiqtn3l2Q04akSU=" crossorigin=anonymous></script></head><body><div class=blog-masthead><div class=container><nav class="nav blog-nav"><a class=nav-link href=https://blog.licenser.net/>Home</a></nav></div></div><header class=blog-header><div class=container><h1 class=blog-title dir=auto><a href=https://blog.licenser.net/ rel=home>Lice!</a></h1><p class="lead blog-description" dir=auto>A collection of somewhat random thoughts.</p></div></header><div class=container><div class=row><div class="col-sm-8 blog-main"><article class=blog-post><header><h2 class=blog-post-title dir=auto><a href=https://blog.licenser.net/2020/01/multithreaded-rust-on-threadripper/>Multithreaded rust on Threadripper</a></h2><p class=blog-post-meta><time datetime=2020-01-11T00:00:00Z>2020-01-11</time> by Heinz N. Gies in
<span class="fas fa-tag" aria-hidden=true></span>&nbsp;<a href=/tags/rust/ rel=tag>rust</a></p></header><p>I recently ran some benchmarks on a Threadripper 3960X system and the results were surprising me quite a bit. Simplified, the throughput the benchmark recorded went down, from 341 MB/s on a MBP to 136 MB/s on a Threadripper desktop. Prior I had read Daniel Lemire's notes on <a href=https://lemire.me/blog/2019/12/05/instructions-per-cycle-amd-versus-intel/>the sub optimal performance for simdjson on Zen 2</a>, which is heavily used in the benchmark, but the suggested drop were a few percent not half.</p><p>Long story short, this made me curious what caused this. First stop: perf.</p><p><img src=/images/tr-rust-perf1.png alt="perf crossbeam"></p><p>Notice the first item? It is <code>crossbeam_channel::flavors::array::Channel&lt;T>::recv</code>. Oh my, I never saw that one hogging so much cpu time, in fact we spend more time in receiving from the channel then we spend in parsing or serializing JSON!</p><p>Lets add a bit of Threadripper trivia, the design AMD went with was splitting the CPU from a single silicon to multiple small dies, they call CCDs with in turn are consists of two CCXs that then contain the cores and level 1-3 cache. So lets look at another thing, htop (trusty little tool to show our load):</p><p><img src=/images/tr-rust-htop1.png alt="htop load on cores"></p><p>In this screenshot we can spot that one thread seems to be running on the 5th core, one on the 16th and one on the 19th and 20th. Thinking back to the design of the Threadripper this is a bit of a hint, those cores are on different CCXs and even further on different CCDs so what happens if they were on the same?</p><p>Boom 400+ MB/s! <code>taskset -c 0,1,2</code> does the trick, that's a really nice improvement and looking at the perf output we can see <code>recv</code> to move from nearly 11% of CPU time to <code>7.28%</code>, now that's a neat improvement. Not only is it nearly 3x faster then the first benchmark but also is it 20% faster then on the laptop. So far so good.</p><p><img src=/images/tr-rust-perf2.png alt="perf crossbeam"></p><p>But it's still leaves the question, why and if we can do something about this. Enter a <a href=https://github.com/Licenser/trb>little benchmark</a> and look at what it puts out for the first core (it's a lot of output otherwise).</p><pre><code>B 0 -  0: -
B 0 -  1: 818us/send
B 0 -  2: 673us/send
B 0 -  3: 2839us/send
B 0 -  4: 2421us/send
B 0 -  5: 2816us/send
B 0 -  6: 3466us/send
B 0 -  7: 3634us/send
B 0 -  8: 3267us/send
B 0 -  9: 3042us/send
B 0 - 10: 3633us/send
B 0 - 11: 3535us/send
B 0 - 12: 3334us/send
B 0 - 13: 3443us/send
B 0 - 14: 3348us/send
B 0 - 15: 3398us/send
B 0 - 16: 3459us/send
B 0 - 17: 3108us/send
B 0 - 18: 3287us/send
B 0 - 19: 3393us/send
B 0 - 20: 3369us/send
B 0 - 21: 3248us/send
B 0 - 22: 3290us/send
B 0 - 23: 3323us/send

B 0 - 24: 487us/send
B 0 - 25: 812us/send
B 0 - 26: 676us/send
B 0 - 27: 2859us/send
B 0 - 28: 2853us/send
B 0 - 29: 2864us/send
B 0 - 30: 3475us/send
B 0 - 31: 3620us/send
B 0 - 32: 3582us/send
B 0 - 33: 3497us/send
B 0 - 34: 3524us/send
B 0 - 35: 3488us/send
B 0 - 36: 3331us/send
B 0 - 37: 3303us/send
B 0 - 38: 3365us/send
B 0 - 39: 3333us/send
B 0 - 40: 3324us/send
B 0 - 41: 3363us/send
B 0 - 42: 3554us/send
B 0 - 43: 3351us/send
B 0 - 44: 3207us/send
B 0 - 45: 3240us/send
B 0 - 46: 3377us/send
B 0 - 47: 3275us/send

</code></pre><p>First things first, the numbers here are 0 indexed, unlike in htop where they're 1 indexed. So core 0 here means core 1 in htop. The test runs only for a second per core combination (as it goes through all cores and otherwise takes a really long time), some variation is to be expected. That gets really slow really fast. We can see that core 24-47 are the SMTs cores on the physical cores 0-23, so 24 being the second thread on core 0. The second observation is that core 0-2 are in the same CCX, performance is reasonable fast here. 3-5 seem to be on the same CCD and so on.</p><p>Lets look at the code for the <a href=https://github.com/crossbeam-rs/crossbeam/blob/2c10be9a54196ac7cbaa068d911a382ed014aa76/crossbeam-channel/src/flavors/array.rs>crossbeam channel</a>. The part that's interesting is that both <code>head</code> and <code>tail</code> are wrapped in <code>CachePadded</code>. Fortunately I have a <a href=https://twitter.com/darachennis>friend</a> who keeps going on about false sharing whenever performance becomes a topic so that was a really good hint here. Looking through the struct aligning head and tail to the cache line makes a lot of sense they're frequently accessed from both sides of the queue but there is another part that's frequently used on both sides. The buffer, and that is just an array of <code>T</code> so it might not align well to the cache. In other words, if we access <code>buffer[x]</code> we might invalidate <code>buffer[x-1]</code> or <code>buffer[x+1]</code> (or more). So what happens if <a href=https://github.com/crossbeam-rs/crossbeam/pull/462>we wrap the the elements in a CachePadded?</a>. The result looks quite nice, it cut down by 50% when going over CCX boundaries:</p><pre><code>B 0 -  0: -
B 0 -  1: 630us/send
B 0 -  2: 678us/send
B 0 -  3: 1319us/send
B 0 -  4: 1256us/send
B 0 -  5: 1291us/send
B 0 -  6: 1438us/send
B 0 -  7: 1504us/send
B 0 -  8: 1525us/send
B 0 -  9: 1660us/send
B 0 - 10: 1772us/send
B 0 - 11: 1807us/send
B 0 - 12: 1382us/send
B 0 - 13: 1380us/send
B 0 - 14: 1387us/send
B 0 - 15: 1375us/send
B 0 - 16: 1382us/send
B 0 - 17: 1383us/send
B 0 - 18: 1471us/send
B 0 - 19: 1471us/send
B 0 - 20: 1463us/send
B 0 - 21: 1462us/send
B 0 - 22: 1468us/send
B 0 - 23: 1457us/send

B 0 - 24: 466us/send
B 0 - 25: 619us/send
B 0 - 26: 671us/send
B 0 - 27: 1438us/send
B 0 - 28: 1422us/send
B 0 - 29: 1514us/send
B 0 - 30: 1789us/send
B 0 - 31: 1688us/send
B 0 - 32: 1812us/send
B 0 - 33: 1820us/send
B 0 - 34: 1719us/send
B 0 - 35: 1797us/send
B 0 - 36: 1383us/send
B 0 - 37: 1364us/send
B 0 - 38: 1373us/send
B 0 - 39: 1383us/send
B 0 - 40: 1370us/send
B 0 - 41: 1390us/send
B 0 - 42: 1468us/send
B 0 - 43: 1467us/send
B 0 - 44: 1464us/send
B 0 - 45: 1463us/send
B 0 - 46: 1475us/send
B 0 - 47: 1467us/send
</code></pre><p>With all of this, the code went from 136 MB/s to over 150 MB/s when not pinned to cores, while this isn't close to where I'd like to to be, it is a 10% improvement in throughput. And looking at perf again recv is completely gone from the list, which is nice!</p><p><img src=/images/tr-rust-perf3.png alt="perf crossbeam"></p><p>This is the conclusion for now, if I have more interesting finds I'll add a continuation - so I'll keep digging.</p><hr><footer><section><h4>Share</h4><nav class="nav sharing-icons"><a class=nav-item href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.licenser.net%2f2020%2f01%2fmultithreaded-rust-on-threadripper%2f" title="Share on Facebook"><span class="fab fa-facebook-f fa-2x" aria-hidden=true></span></a><a class=nav-item href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fblog.licenser.net%2f2020%2f01%2fmultithreaded-rust-on-threadripper%2f" title="Share on LinkedIn"><span class="fab fa-linkedin-in fa-2x" aria-hidden=true></span></a><a class=nav-item href="https://twitter.com/intent/tweet?url=https%3a%2f%2fblog.licenser.net%2f2020%2f01%2fmultithreaded-rust-on-threadripper%2f&text=Multithreaded%20rust%20on%20Threadripper" title="Tweet this"><span class="fab fa-twitter fa-2x"></span></a></nav></section><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"lice"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></footer></article></div><aside class="col-sm-3 ml-auto blog-sidebar"><section class=sidebar-module><h4>Links</h4><ol class=list-unstyled><li><a href=https://github.com/Licenser>GitHub</a></li></ol></section></aside></div></div><footer class=blog-footer><p dir=auto>Blog template created by <a href=https://twitter.com/mdo>@mdo</a>, ported to Hugo by <a href=https://twitter.com/mralanorth>@mralanorth</a>.</p><p><a href=#>Back to top</a></p></footer></body></html>