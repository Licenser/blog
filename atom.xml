<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Lice!]]></title>
  <link href="http://blog.licenser.net/atom.xml" rel="self"/>
  <link href="http://blog.licenser.net/"/>
  <updated>2020-01-11T22:07:07+01:00</updated>
  <id>http://blog.licenser.net/</id>
  <author>
    <name><![CDATA[Heinz N. 'Licenser' Gies]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Multithreaded Rust on Threadripper]]></title>
    <link href="http://blog.licenser.net/blog/2020/01/11/multithreaded-rust-on-threadripper/"/>
    <updated>2020-01-11T14:18:43+01:00</updated>
    <id>http://blog.licenser.net/blog/2020/01/11/multithreaded-rust-on-threadripper</id>
    <content type="html"><![CDATA[<p>I recently ran some benchmarks on a Threadripper 3960X system and the results were surprising me quite a bit. Simplified, the throughput the benchmark recorded went down, from 341 MB/s on a MBP to 136 MB/s on a Threadripper desktop. Prior I had read Daniel Lemire&rsquo;s notes on <a href="https://lemire.me/blog/2019/12/05/instructions-per-cycle-amd-versus-intel/">the sub optimal performance for simdjson on Zen 2</a>, which is heavily used in the benchmark, but the suggested drop were a few percent not half.</p>

<p>Long story short, this made me curious what caused this. First stop: perf.</p>

<p><img src="http://blog.licenser.net/images/tr-rust-perf1.png" alt="perf crossbeam" /></p>

<p>Notice the first item? It is <code>crossbeam_channel::flavors::array::Channel&lt;T&gt;::recv</code>. Oh my, I never saw that one hogging so much cpu time, in fact we spend more time in receiving from the channel then we spend in parsing or serializing JSON!</p>

<p>Lets add a bit of Threadripper trivia, the design AMD went with was splitting the CPU from a single silicon to multiple small dies, they call CCDs with in turn are consists of two CCXs that then contain the cores and level 1-3 cache. So lets look at another thing, htop (trusty little tool to show our load):</p>

<p><img src="http://blog.licenser.net/images/tr-rust-htop1.png" alt="htop load on cores" /></p>

<p>In this screenshot we can spot that one thread seems to be running on the 5th core, one on the 16th and one on the 19th and 20th. Thinking back to the design of the Threadripper this is a bit of a hint, those cores are on different CCXs and even further on different CCDs so what happens if they were on the same?</p>

<p>Boom 400+ MB/s! <code>taskset -c 0,1,2</code> does the trick, that&rsquo;s a really nice improvement and looking at the perf output we can see <code>recv</code> to move from nearly 11% of CPU time to <code>7.28%</code>, now that&rsquo;s a neat improvement. Not only is it nearly 3x faster then the first benchmark but also is it 20% faster then on the laptop. So far so good.</p>

<p><img src="http://blog.licenser.net/images/tr-rust-perf2.png" alt="perf crossbeam" /></p>

<p>But it&rsquo;s still leaves the question, why and if we can do something about this. Enter a <a href="https://github.com/Licenser/trb">little benchmark</a> and look at what it puts out for the first core (it&rsquo;s a lot of output otherwise).</p>

<pre><code>B 0 -  0: -
B 0 -  1: 818us/send
B 0 -  2: 673us/send
B 0 -  3: 2839us/send
B 0 -  4: 2421us/send
B 0 -  5: 2816us/send
B 0 -  6: 3466us/send
B 0 -  7: 3634us/send
B 0 -  8: 3267us/send
B 0 -  9: 3042us/send
B 0 - 10: 3633us/send
B 0 - 11: 3535us/send
B 0 - 12: 3334us/send
B 0 - 13: 3443us/send
B 0 - 14: 3348us/send
B 0 - 15: 3398us/send
B 0 - 16: 3459us/send
B 0 - 17: 3108us/send
B 0 - 18: 3287us/send
B 0 - 19: 3393us/send
B 0 - 20: 3369us/send
B 0 - 21: 3248us/send
B 0 - 22: 3290us/send
B 0 - 23: 3323us/send

B 0 - 24: 487us/send
B 0 - 25: 812us/send
B 0 - 26: 676us/send
B 0 - 27: 2859us/send
B 0 - 28: 2853us/send
B 0 - 29: 2864us/send
B 0 - 30: 3475us/send
B 0 - 31: 3620us/send
B 0 - 32: 3582us/send
B 0 - 33: 3497us/send
B 0 - 34: 3524us/send
B 0 - 35: 3488us/send
B 0 - 36: 3331us/send
B 0 - 37: 3303us/send
B 0 - 38: 3365us/send
B 0 - 39: 3333us/send
B 0 - 40: 3324us/send
B 0 - 41: 3363us/send
B 0 - 42: 3554us/send
B 0 - 43: 3351us/send
B 0 - 44: 3207us/send
B 0 - 45: 3240us/send
B 0 - 46: 3377us/send
B 0 - 47: 3275us/send
</code></pre>

<p>First things first, the numbers here are 0 indexed, unlike in htop where they&rsquo;re 1 indexed. So core 0 here means core 1 in htop. The test runs only for a second per core combination (as it goes through all cores and otherwise takes a really long time), some variation is to be expected. That gets really slow really fast. We can see that core 24-47 are the SMTs cores on the physical cores 0-23, so 24 being the second thread on core 0. The second observation is that core 0-2 are in the same CCX, performance is reasonable fast here. 3-5 seem to be on the same CCD and so on.</p>

<p>Lets look at the code for the <a href="https://github.com/crossbeam-rs/crossbeam/blob/2c10be9a54196ac7cbaa068d911a382ed014aa76/crossbeam-channel/src/flavors/array.rs">crossbeam channel</a>. The part that&rsquo;s interesting is that both <code>head</code> and <code>tail</code> are wrapped in <code>CachePadded</code>. Fortunately I have a <a href="https://twitter.com/darachennis">friend</a> who keeps going on about false sharing whenever performance becomes a topic so that was a really good hint here. Looking through the struct aligning head and tail to the cache line makes a lot of sense they&rsquo;re frequently accessed from both sides of the queue but there is another part that&rsquo;s frequently used on both sides. The buffer, and that is just an array of <code>T</code> so it might not align well to the cache. In other words, if we access <code>buffer[x]</code> we might invalidate <code>buffer[x-1]</code> or <code>buffer[x+1]</code> (or more). So what happens if <a href="https://github.com/crossbeam-rs/crossbeam/pull/462">we wrap the the elements in a CachePadded?</a>. The result looks quite nice, it cut down by 50% when going over CCX boundaries:</p>

<pre><code>B 0 -  0: -
B 0 -  1: 630us/send
B 0 -  2: 678us/send
B 0 -  3: 1319us/send
B 0 -  4: 1256us/send
B 0 -  5: 1291us/send
B 0 -  6: 1438us/send
B 0 -  7: 1504us/send
B 0 -  8: 1525us/send
B 0 -  9: 1660us/send
B 0 - 10: 1772us/send
B 0 - 11: 1807us/send
B 0 - 12: 1382us/send
B 0 - 13: 1380us/send
B 0 - 14: 1387us/send
B 0 - 15: 1375us/send
B 0 - 16: 1382us/send
B 0 - 17: 1383us/send
B 0 - 18: 1471us/send
B 0 - 19: 1471us/send
B 0 - 20: 1463us/send
B 0 - 21: 1462us/send
B 0 - 22: 1468us/send
B 0 - 23: 1457us/send

B 0 - 24: 466us/send
B 0 - 25: 619us/send
B 0 - 26: 671us/send
B 0 - 27: 1438us/send
B 0 - 28: 1422us/send
B 0 - 29: 1514us/send
B 0 - 30: 1789us/send
B 0 - 31: 1688us/send
B 0 - 32: 1812us/send
B 0 - 33: 1820us/send
B 0 - 34: 1719us/send
B 0 - 35: 1797us/send
B 0 - 36: 1383us/send
B 0 - 37: 1364us/send
B 0 - 38: 1373us/send
B 0 - 39: 1383us/send
B 0 - 40: 1370us/send
B 0 - 41: 1390us/send
B 0 - 42: 1468us/send
B 0 - 43: 1467us/send
B 0 - 44: 1464us/send
B 0 - 45: 1463us/send
B 0 - 46: 1475us/send
B 0 - 47: 1467us/send
</code></pre>

<p>With all of this, the code went from 136 MB/s to over 150 MB/s when not pinned to cores, while this isn&rsquo;t close to where I&rsquo;d like to to be, it is a 10% improvement in throughput. And looking at perf again recv is completely gone from the list, which is nice!</p>

<p><img src="http://blog.licenser.net/images/tr-rust-perf3.png" alt="perf crossbeam" /></p>

<p>This is the conclusion for now, if I have more interesting finds I&rsquo;ll add a continuation - so I&rsquo;ll keep digging.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dell XPS/Windows as a Dev Env]]></title>
    <link href="http://blog.licenser.net/blog/2018/05/21/dell-xps-slash-windows-as-a-dev-env/"/>
    <updated>2018-05-21T21:19:34+02:00</updated>
    <id>http://blog.licenser.net/blog/2018/05/21/dell-xps-slash-windows-as-a-dev-env</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve recently gotten a Dell XPS 15" 2-in-1 and started using it as a development environment for the last week. To be honest as a long term MacBook user I expected a rather disapointing experience but to my big surprise I do really like it so far. But enough of a preamble. Why I&rsquo;m writing this? Because I figured that the mistakes I made, the hints I got all over the place would have really helped me if someone had collected them - so I do that now. Mind you a lot of it is not specific to the Dell and will work for every system.</p>

<h2>The goal</h2>

<p>Let&rsquo;s set expecations. This is not about making the perfect system, after all a week isn&rsquo;t enough to decide what perfect is and find out all the little things that need to get tweeked. The goal here is to get a &lsquo;good enough&rsquo; system that works for most everything without regretting it or missing my MacBook in as little time as possible.</p>

<h2>The OS</h2>

<p>I suspect many people will be surprised but I decided to stay with windows as a operating system, partially because I&rsquo;ve not the best opinion on Linux (which I&rsquo;m not going to discuss here) and partially because I wanted to see how good the out of the box experience is. Last but not least the whole 2-in-1 experience seems to be excelently integreated with the OS and I&rsquo;m a bit scared to have to fight a different OS to get it all working as it&rsquo;d be two unknowns to fight instead of just one.</p>

<p>WSL is brilliant, I&rsquo;ve had very little problems (read none but one super esoteric one we&rsquo;ll get to later) with it.</p>

<h2>My setp</h2>

<p>To my great fortune I do use Eemacs which means that my environment of choice works nearly everywhere. OSX, Linux, Windows, WSL, there is an Emacs. Other then that I obviously need my Erlang, I really can&rsquo;t go without it ;). In addition I tossed in some rust and since I&rsquo;m writing this post on the XPS ruby. The usual suspects, gcc, make, git and friends come with the terretory.</p>

<p>My initial attempt was to run things on windows. I tried VS code and windos emacs. Erlang, rust, git all exist as native windows binaries but the whole experience wasn&rsquo;t great especially since in the end it&rsquo;ll run on a unix or unix like system anyhow. So in the end I used WSL for help and just set everything up there.</p>

<h3>Ubuntu 18.04</h3>

<p>The WSL installation I picked. I don&rsquo;t want to go into a distribution war and I&rsquo;m sure all other distributions just work as well - pick your favourite I simply know ubuntu a bit better then I know SuSe (the alternative from the app store) and can&rsquo;t be bothered to find out how to add additional distributions.</p>

<p>Most of everything comes in <code>apt</code>, with the exception of rust which uses <a href="https://rustup.rs">rustup</a> and erlang wich I fetched the <a href="https://www.erlang-solutions.com/resources/download.html">ESL package</a> for to get around ubuntus horrible decision to split up erlang in multiple packages (seriously WTF?!?).</p>

<h3>Emacs</h3>

<p>I use <a href="http://spacemacs.org/">space emacs</a> which installes without problems on WSL, just clone the reposit and call it a day. It&rsquo;ll ask you for additions for languages as you open the files, a lot exist but if you do something that&rsquo;s more esoteric then erlang you might have to do some additional fiddling.</p>

<p>Here comes the first issue, WSL does not have an X server (I really hope at some point Microsoft will add one) but in the meantime <a href="https://sourceforge.net/projects/vcxsrv/">VcXsrv</a> works quite well.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Does VcXsrv not give you the X you need?</p>&mdash; Social Justice Webserver (@joedevivo) <a href="https://twitter.com/joedevivo/status/997958930570493952?ref_src=twsrc%5Etfw">May 19, 2018</a></blockquote>


<p> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>You can add it to auto-start too which helps! After running it the first time it&rsquo;ll ask you to save the config, do so and then run windows-r and enter <code>shell:startup</code> to open the folder for auto-start, drop the file in there and call it a day.</p>

<p>Once <code>X</code> is started you need to tell the WSL shell where your display is you can do that by adding the line <code>export DISPLAY=localhost:0</code> at the end of the <code>~/.profile</code> file.</p>

<p>With that all set you should have a running emacs, feel free to skip the the next section or keep reading here if you want to know a few more fancies.</p>

<p>I really like the <a href="https://github.com/tonsky/FiraCode">Fira Code</a> font, fortunately it comes with ubuntu so you can install it by running <code>sudo install fonts-firacode</code>. To enable it in emacs you can follow <a href="https://github.com/tonsky/FiraCode/wiki/Emacs-instructions">their tutorial</a>. Here is the gotcha I talked about while the font works I&rsquo;ve not yet managed to get the code points to work, but it&rsquo;s not that big of a deal to me.</p>

<p>A second fancy but not required thing is a Emacs desktop icon. I really wanted the option to just click it and make it start without having to go through bash and running a command or having a console window open. There is a simple trick for that. Create a VB script somewhere with the follow content:</p>

<pre><code class="basic">Set oShell = CreateObject ("Wscript.Shell") 
Dim strArgs
strArgs = "bash -c ~/start-emacs"
oShell.Run strArgs, 0, false
</code></pre>

<p>and a shell script in your home directory called start-emacs:</p>

<pre><code class="bash">#!/bin/sh

export DISPLAY=localhost:0
emacs
</code></pre>

<p>That&rsquo;s it you can then link that to the laptop and give it the nice <a href="https://raw.githubusercontent.com/nashamri/spacemacs-logo/master/spacemacs.ico">spacemacs icon</a>.</p>

<h3>Windows / The hardware</h3>

<p>There are a few tweaks to the system I hade to make it more frinedly to my use (read MY use, so YMMV).</p>

<h3>Touchpad</h3>

<p>The touchpad just isn&rsquo;t at par with Apples touchpad, honestly none is and I suspect if you ever used a MacBook&rsquo;s touchpad you easiely agree. But XPS is one of the better ones, probably the best on a PC I&rsquo;ve ever used and there are a few tweaks that make it more user friendly.</p>

<ol>
<li>Disable tap to click, it&rsquo;s just super annoying if you&rsquo;re used the Mac touchpad.</li>
<li>disable the right side as a right click, two fingers like on the Mac just work fine and feel more natura (to a mac user).</li>
<li>Set 3 finger gestures to &ldquo;Switching Desktops and showing desktops&rdquo;.</li>
</ol>


<h3>Keyboad</h3>

<p>The XPS uses what dell calls a maglev keyboard, which sounds really cool, it also types quite well. It&rsquo;s a bit clicky, that&rsquo;s not for everyone but pressing the key is distinct and you don&rsquo;t get this &ldquo;did I press it or not&rdquo; feeling. Not good or bad but I noticed that compared to the Mac the alt and ctrl keys are placed different. While on the Mac FN and Command/apple/windows are on the outside and alt and ctrl are on the inside the Dell does it the oposite way around. It takes some time to get used to but I think after a few days the way it&rsquo;s on the Dell feels more natural and better reachable (I still press Windows-D instead of Alt-D way too often :P).</p>

<p>Now the downer. Dell has made the horrible deicsion to put the page up/down keys right about the arrow keys. I hate it. It&rsquo;s unnatural and trying to count the time I went a page up instead of left resulted in an integer overflow. Fortunately Kevin suggested a toll called <a href="https://www.autohotkey.com/">AutoHotkey</a>. A simple script like this:</p>

<pre><code>PgUp::Left
PgDn::Right
</code></pre>

<p>sovles the problem. AHK translates it to a standalone executable that you can put into the auto-start folder mentioned above. Effectively this re-binds the page up/down keys to left/right so that no matter which you end up hitting you get the arrow keys.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Migrating to Rebar3]]></title>
    <link href="http://blog.licenser.net/blog/2015/07/10/migrating-to-rebar3/"/>
    <updated>2015-07-10T01:11:12+02:00</updated>
    <id>http://blog.licenser.net/blog/2015/07/10/migrating-to-rebar3</id>
    <content type="html"><![CDATA[<h1>A long journey from rebar2 to rebar3</h1>

<p>Rebar 3 has recently started to surface out of alpha state and entered beta1, about time for the crazy people like me to abandon tried and tested tools to venture into the great vastness of the unknown!</p>

<p>So with a backpack, hiking shoes, food for about a week and a direct line to the <code>rebar3</code> IRC channel I set off to migrate <a href="https://github.com/project-fifo/sniffle">sniffle</a> from <code>rebar2</code> to <code>rebar3</code>. Now, after it looks like everything is working, I want to write up what exactly went down.</p>

<p>The complete delta can be seen <a href="https://github.com/project-fifo/sniffle/pull/2/files?diff=split">here</a> please be ware that the upgrade kicked of a bit of a chain reaction with updating libraries too.</p>

<h1>3 =/= 2 + 1</h1>

<p>The most important thing I found, or rather the biggest misconception I had, is that <code>rebar3</code> is the next iteration of <code>rebar2</code>. This lead to a lot of misery on my part. <code>rebar3</code> is an entirely different application, the workflow is different, the logic is different and the behavior is different. Just dropping it and expecting everything to keep working the same will not end well. Treat it like migrating to a different build tool and things are a lot easier.</p>

<h1>The simple stuff</h1>

<h2>Folders and files</h2>

<p>Some of the directories change, <code>deps</code> no longer exists and <strong>has to be deleted</strong>. It also can be removed form the <code>.gitignore</code> file. Instead <code>_build</code> now takes its place, somewhat, it&rsquo;s different but it can go into <code>.gitignore</code>.</p>

<p>The same way <code>ebin</code> doesn&rsquo;t exist any longer and <strong>should be deleted</strong>. The former <code>ebin</code> now lives also in <code>_build</code> so we don&rsquo;t need to add anything new to the <code>.gitignore</code> file here. The old <code>ebin</code> will take priority over the <code>.beam</code> files generated in <code>_build</code>. That said I was pointed to the fact that there are valid reasons to have it around, for example to prevent <code>rebar3</code> to generate the <code>.app</code> file from <code>.app.src</code> or if there are additional files compiled by another tool other then <code>rebar3</code>.</p>

<p>Now I said and even said it in bold that those folders <strong>HAVE TO BE DELETED</strong> that is because they do, if not an axe murderer will come by your house and kill your cat, seriously, I was just lucky that I didn&rsquo;t have a cat so he left disappointed. Aside of the axe murderer, <code>rebar3</code> will also rather unexpectedly load things from there, which, unlike then the axe murderer, did affect me and cause quite some headache.</p>

<p>There is a new file, <code>rebar.lock</code> which you want to add to your repository, not ignore it, it will keep track of the versions of libraries that are into the <code>_build</code> directory and that should go there if they don&rsquo;t already exist.</p>

<h2>Commands</h2>

<p>The <code>rebar get-deps</code> is deprecated so is <code>rebar update-deps</code>, you don&rsquo;t need them any more, <code>rebar3</code> figures out itself when dependencies need to be installed or updated (from the <code>rebar.lock</code> file). There is a new <code>rebar3 deps</code> command, which has nothing to do with the old commands, instead it is used to give a list of the dependencies of your project (but not the sub-dependencies).</p>

<p><code>rebar doc</code> is now called <code>rebar3 edoc</code> that should be noted.</p>

<p><code>rebar3 dialyzer</code> is new, it replaces the old workflow of running dialyzer on its own and does all the building and checking <code>.ptl</code> files for you. The old trick of <code>grep</code>-ing away known errors to mitigate them is not working due to the mixed output however I was told that erlang 18 comes with a <a href="http://www.erlang.org/doc/man/dialyzer.html"><code>-dializer</code> pre-compiler directive</a> can be used to handle this. I am not really sure about this especially with third party libraries.</p>

<p>The handling of dependencies changed too, <code>skip_deps=true</code> is no longer needed. Nor is <code>-r</code> if you are using a <code>apps/*/...</code> structure for your project. Along with those the <code>-D</code> flag is gone now, it can however the same can be achieved with profiles - later more to that.</p>

<p><code>generate</code> was replaced by <code>release</code> and it now uses <code>relx</code> and not <code>reltool</code>. If you are one of the poor sods (like me) that was using <code>reltool</code> you are into a lot more fun here but that is mostly beyond the scope. If you used <code>relx</code> before this should be straight forward, just that the config now lives in the <code>rebar.config</code>. Existing <code>relx.config</code> files will still be used as long as no <code>relx</code> section exists. It should be noted that this also takes care of linking instead of copying files when used with <code>{dev_mode, true}</code> which can be very nice for developing.</p>

<h2>Profiles</h2>

<p>Now there is probably a lot to say, it is a way to handle differences in behavior, and can for example replace the <code>-D</code> flag like this: <code>{profiles, [{long, {erl_opts, [{d, LONGTESTS}]}</code>. I haven&rsquo;t fully grasped the power and best practice of this and there is a good <a href="http://www.rebar3.org/docs/profiles">article in the docs</a> about this so I won&rsquo;t dive further into that.</p>

<p>Something worth pointing out before moving on is that <strong>everything</strong> that can be in a <code>rebar.config</code> can be in a profile, including plugins, dependencies, erlang options and so on. This makes it incredibly powerful.</p>

<h2>Plugins</h2>

<p><a href="http://www.rebar3.org/docs/using-available-plugins">Plugins</a> have changed a bit and become a lot more important. Some common tasks in <code>rebar2</code> now live in a plugin instead of being part of the core system. The most notable here is probably the Port Compiler (or <code>pc</code> as the plugin is called) which is used for building NIFs (like eleveldb).</p>

<p>Hex comes as a plugin, which is <strong>really</strong> nice, however the plugin is needed to publish not to fetch dependencies. This plugin could happily go into the global config, yes there is a global config in <code>~/.config/rebar3/rebar.config</code>. However it is best to keep other plugins out there.</p>

<p>The EQC (QuickCheck) plugin is very nice if you have quick check, either the free or the commercial version. It should be pointed out here <strong>not</strong> to put this in the global config, no matter how tempting it is or the axe murderer will come back. Other then that you can now put the properties into a <code>eqc</code> folder and separate them from tests and it is no longer needed to wrap them in <code>-ifdef(EQC)</code>  and <code>-ifdef(TEST)</code>. What is especially nice here is that it picks up on the same naming as <a href="http://quickcheck-ci.com/">quickcheck-ci</a> so that will make things easier.</p>

<h2>Dependencies</h2>

<p>This is a quite big topic, but it can be summed up in: forget everything you know about rebar&rsquo;s handling of dependencies it&rsquo;s invalid now.</p>

<p>Perhaps the most obvious change is that in addition to source dependencies you can now include hex packages. The packages can take the form: <code>dflow</code> as &lsquo;the latest version&rsquo; (or the version fitting to other packages), or as <code>{dflow, "0.1.6"}</code> to pick a specific version (more details <a href="http://elixir-lang.org/docs/stable/elixir/Version.html">here</a>).</p>

<p>Using packages has a huge advantage, they are cached locally which makes fetching them, especially for big projects, <strong>a lot nicer</strong>.</p>

<p>Now my experience with rebar2 was that dependencies were handled by just cloning all of the dependencies in the <code>deps</code> folder and then adding them to the library path. This also had the effect that order didn&rsquo;t really matter. For example you could happily include header files from projects you were not depending on in the application including it.</p>

<p>Now rebar3 is actually caring about what you do. For example I ran into the following situation. I have an application <code>sniffle</code> this application has file <code>include/sniffle_version.hrl</code>. Now <code>sniffle</code> was depending on <code>sniffle_watchdog</code>, however <code>sniffle_watchdig</code> was including <code>include/sniffle_version.hrl</code></p>

<pre><code>     +-------------+             +-------------+
     |   sniffle   |&lt;------------|  watchdog   |
     +-------------+             +-------------+
            |                           ^
            |                           |
            |                           |
            |                           |
            |                           |
+-----------------------+               |
|  sniffle_version.hrl  |---------------+
+-----------------------+
</code></pre>

<p>This setup is no problem with <code>rebar2</code>, those files where in <code>apps/sniffle/include</code> and works great the file exist and that is all that&rsquo;s needed. <strong>However</strong>, with rebar3 this approach is problematic, since  <code>sniffle_watchdog</code> does not depend on <code>sniffle</code> it will not exist when <code>sniffle_watchdog</code> is compiled. This means that I needed to include <code>sniffle</code> in <code>sniffle_watchdog</code> which is not possible since it would create a circlular dependency. The solution for this was simply to put the version header in na own application that gets included into both.</p>

<pre><code>+-------------+             +-------------+
|   sniffle   |&lt;------------|  watchdog   |
+-------------+             +-------------+
       ^                           ^
       |                           |
       |                           |
       |                           |
       | +-----------------------+ |
       +-|    sniffle_version    |-+
         +-----------------------+
</code></pre>

<p>Another slightly related topic is that when building releases now the content of the app file matters more, that is probably my own shortcoming that I ran into the problem but I did not include many library applications into the <code>application</code> section of the <code>.app.src</code> file. That lead to them missing in the release and the application dying a horribly painful death. I found the following code snipped rather helpful to track what applications were missing, and then a lot of manual labour to find where they should be included.</p>

<pre><code class="bash">ls -1 _build/default/rel/sniffle/lib/  | sed 's/-.*//g' | sort &gt; rlibs
ls -1 _build/default/lib | sort &gt; libs
vimdiff libs rlibs
</code></pre>

<h2>The bottom line</h2>

<p>After working with it a bit I think rebar3, when treated as it&rsquo;s own tool and not a iteration of rebar, is going to be huge improvement over existing erlang build tools, both erlang.mk, rebar2, and most likely any of the others lurking in the shadows.</p>

<p>The devs are very friendly and responsive and have helped me a great deal during this rather interesting exercise and deserve a lot of credit for the work and for putting up with the involved hatred and anger they receive.</p>

<p>Yes rebar3 is a learning curve and in the beginning it can be quite steep, but so does any other tool to be fair. It still is in beta (for a good reason), but bugs are fixed very fast and the help debugging them is outstanding.</p>

<p>If you require a rock solid tool today it is probably best to wait a bit longer until the final release but that said I have come a full circle, from utter hatred and frustration (on day one) to loving it after a week and will be using it from now on.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postmortem of a Interesting Bug]]></title>
    <link href="http://blog.licenser.net/blog/2014/11/28/postmortem-of-a-interesting-bug/"/>
    <updated>2014-11-28T19:20:34+01:00</updated>
    <id>http://blog.licenser.net/blog/2014/11/28/postmortem-of-a-interesting-bug</id>
    <content type="html"><![CDATA[<h2>Symptoms</h2>

<p>After a full network outage in a larger system (7 FiFo instances and, a few dozen of hypervisors, VM&rsquo;s in the 3 digit number) a small percentage of the VM&rsquo;s stored in FiFo lost information which package was assigned to them and which organization they belong to.</p>

<h2>Technical background</h2>

<p>As part of planned maintenance on the switching layer the entire network was taken down. Effectively cutting the communication between any two systems in the network.  During the maintenance the FiFo system was kept &ldquo;hot&rdquo;, no services disabled or paused. At the end of the maintenance window the network was as planned enabled again, reinstating communications.</p>

<h2>FiFo background</h2>

<p>We will focus in the sniffle and chunter components since those are the relevant parts of sniffle that handle information related to the symptoms.
¯
Generally all fifo data is stored in CRDT&rsquo;s which provides conflict resolution and nearly loss free merging even of entirely divergent data.</p>

<h3>Sniffle</h3>

<p>Sniffle is a distributed system that runs on top of riak_core, all (7) nodes are connected as a mash network via distributed erlang. While the nodes are connected data and functionality is split out in dynamo fashion, at a default N value of 3 that means every node handles (total data)*3/7 of the data.</p>

<p>In the case of a node failure the adjacent node takes over the function of the failed node. Missing data is resolved by a method called &ldquo;read repair&rdquo;, meaning that when data is requested and 1 out of the 3 nodes responds with no data that &ldquo;missing&rdquo; data is repaired.</p>

<p>Once the node failure is repaired the system that took over the work for the failed node performs what is called a handoff, sending it&rsquo;s (supposedly) updated data to the returning node to bring it up to speed.</p>

<h3>Chunter</h3>

<p>Under normal conditions chunter will send incremental updates about changing vm state to sniffle. If a chunter system for the first time connects to sniffle it will register it&rsquo;s VM&rsquo;s, meaning they are created if not existent, and assigned to the hypervisor. The registration contains nearly all relevant VM information with some exceptions that do not concern the hypervisor. The missing information was amongst the not updated information.</p>

<p>Sniffle nodes are discovered via mDNS broadcasts, the first broadcast received will trigger the initial registration sending the request to this node.</p>

<h2>What happened?</h2>

<p>It is a combination of conditions that lead to the problem and explains why  only a small percentage of VM&rsquo;s were affected.</p>

<h3>During the outage</h3>

<p>1) The network outage cut the communication between <strong>all</strong> sniffle nodes, so each node was on its own handling the full load of the requests.
2) The network outage cut communication of <strong>nearly all</strong> chunter nodes, only those who resided on a hypervisor that also contained a fifo zone did not loose connectivity.</p>

<h3>After the outage</h3>

<p>Now there happened a race condition, on one side the distributed erlang started to re-discover the other nodes, reestablishing communication in sniffle and initializing handoffs (handoffs of nearly all parts).</p>

<p>On the other side the periodic mDNS broadcasts from the sniffle nodes triggered re-registration of the chunter nodes.</p>

<h3>The bug</h3>

<p>The bug was triggered by the fact that during receiving a handoff a vnode took the wrong assumption that the received data must be newer then its own and overwrite the currently stored data instead of merging it.</p>

<p>Due to the usual repair during reads this turned out to be quite an edge case that was only triggered when:</p>

<p>1) The mDNS was received from a node that neither holds the right partition, neither is connected to a node holding the right partition.
2) the registration happened to three vnodes that did not hold that data since as long as one node held the data it would have been merged with this instead of re-created
3) The handoff on all three systems was performed before any read happened, since the read-repair would haver otherwise merged the data.
4) The handoff off all three systems was performed before AAE triggered, since AAE triggers a read-repair on inconsistent data.</p>

<h2>The fix</h2>

<p>This was fixed rather simple, instead of overwriting existing data on a handoff, the handoff is now treated as a read-repair and old and existing data is merged.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Post-mortem of a Failed Support Case.]]></title>
    <link href="http://blog.licenser.net/blog/2014/02/02/post-mortem-of-a-failed-support-case/"/>
    <updated>2014-02-02T05:27:27+01:00</updated>
    <id>http://blog.licenser.net/blog/2014/02/02/post-mortem-of-a-failed-support-case</id>
    <content type="html"><![CDATA[<p>Every now and then I check the link reports for Project FiFo to see what people think and write about it. Recently I stumbled about <a href="http://consulting.miracleas.dk/blog/post/2014/01/17/Quick-and-Dirty-spawned-LightlyCloudy!.aspx">an article</a> that oddly enough made me both proud and sad. It actually was a rather negative one, which is a shame, but on the other hand a project isn&rsquo;t mature until people care enough to complain.</p>

<p>Yet even so it would be very easy to cast this aside as a &lsquo;success&rsquo; in a strange manner, it still bothers me that someone is upset enough with FiFo to spend his time writing a longish blog article and write their own management software. I do take great pride in the fact that we do our best to have an outstanding documentation and a good support for users of FiFo, and I dare to say so does anyone else who is involved in the project.</p>

<p>So armed with the full history of events  plus the additional background from the <a href="http://consulting.miracleas.dk/blog/post/2014/01/17/Quick-and-Dirty-spawned-LightlyCloudy!.aspx">article</a> mentioned above I want to try a post mortal analysis of this support case, what went wrong and how it could be avoided. I obviously will try to be as impartial as I can be but in the end I can only look at what I see. Hopefully there is a lesson to be learned from this so next time things went smoother.</p>

<p>As a disclaimer: the problem is still a mystery and will probably remain so forever. Also all documentation linked is from the time the bug was filed (thanks for versioning) to give a fair picture of what was available back then.</p>

<h1>Act 3 - the mailing list</h1>

<p>The history of this case starts a bit into the history of the whole story, but for now lets skip the first part and look at the first time we (as in the FiFo team) came aware of the problem: <a href="https://groups.google.com/forum/#!topic/project-fifo/MZJYOs-CtP8">A mail to the mailing list and the conversation following it</a>.</p>

<p>Sadly the initial mail gives every little to no information on the problem aside that &lsquo;not firing on all cylinders/Suddenly FiFo stopped working all by it self&rsquo;. I must admit that for me such a thing is a big red flag that the person on the other end does not care much or is not willing to invest energy into finding the problem, looking from today and after reading the articles on Gordon&rsquo;s blog it might as well just have been the kind of humor used, we will never know.</p>

<p>But fortunately I am not the only one on the mailing list and others are slower to judge and have more patience and Mark jumped in to help and tried to help, starting to ask for additional information that were missing from the original mail. After the information not showing any additional evidence that could lead to an easy conclusion Marks ask for him to take a look at <a href="http://project-fifo.net/pages/viewpage.action?pageId=8126470">FiFo&rsquo;s Problem Checklist</a> and come to the IRC channel to help further.</p>

<h2>Act 3 conclusion</h2>

<p>Mark replied to the initial mail within less then an hour which is a truly amazing response time that can put most commercial support to shame request to go through the checklist and join the channel for a more direct help is a very good approach.</p>

<p>Since I can actually look into my head I shed a bit more light on my take at the point, I decided to opt out of that request at that moment since Mark seemed to have it covered and the tone of the mail already made me sort it in a bucket of &lsquo;going to be annoying&rsquo;. So I probably should not judge as fast.</p>

<p>To receive the best possible support there are some things Gordon could have done, the initial mail could have contained more information, the tone could have been different — jokes often communicate badly over written text, and following Marks request to go through the checklist had probably helped too if only to get some additional facts for later.</p>

<h1>Act 4 - IRC</h1>

<p>Marks suggested to swap to IRC for a quicker communication, we all hate mail ping pong I guess. Fortunately we <a href="http://echelog.com/logs/browse/project-fifo/1382914800">keep logs</a> of the FiFo channel to make it easy to google for known problems that were discussed before — in this case it allows us to look at the conversation between Mark (aka trentster) and Gordon (g-flemming).</p>

<p>This is pretty uneventful mark asks a few more questions to rule out common errors none of which seem to be the cause. At this point Mark asks to escalate the issue and file a ticket with the logs and offers the suggestion to move to a newer version of FiFo (the development build at that time) along with <a href="http://project-fifo.net/pages/viewpage.action?pageId=7176245">the manual containing steps necessary to do this</a>. The reasoning being that the development build is quite well tested with multiple people (including Mark and myself) running it.</p>

<h2>Act 4 conclusions</h2>

<p>Things are still pretty well at this point, our escalation process seems to work wonderfully and the resources of the documentation hold a lot of valuable information.</p>

<p>This is where the story ends for Mark and to all honestly he did an astonishing job to support here and escalate when he ran out of ways to help.</p>

<p>On the channel Gordon mentions the first time that he is running FiFo with customer on it this indicates some urgency on the matter and at least I missed that line. Sadly so he did not seem to have went through the troubleshooting steps or read the update documentation.</p>

<p>The lesson to learn here for us is to either ask people to include an IRC log in a new ticket or do it ourselves after they create it, this might have added some additional info to the ticket that was not present when it was created.</p>

<h1>Act 5 - The ticketing system</h1>

<p>Now this is where my part in the story starts, just as before I&rsquo;ll try to give some additional insight on what I was thinking at this time to shed a bit of light on my end - I obviously can&rsquo;t do the same for Gordon.</p>

<p>As asked by Mark Gordon created a <a href="http://jira.project-fifo.net/browse/FIFO-142">JIRA Ticket</a> with the logs. When seeing the ticket I had already read the mailing list but not the IRC backlog (I don&rsquo;t always do that since I don&rsquo;t want to spend too much time reading old things that might not even affect me). From the ML I have already an unhappy feeling towards the issue as it looked to me that Gordon was not willing to put much effort into resolving the issue.</p>

<p>Non the less I take a look at the logs and trie to pice together what exactly happened, up to this date I do not know what it was. The suggestions were pretty close to what Mark suggested earlier, a problem with too little memory or the filesystem the error in the logs <code>einval</code> hinted to some issue with a POSIX filesystem call.</p>

<p>And at this point things are going wrong, lacking the information that there are production users on the system I make the mistake to judge the issue as non urgent especially after the full reply from Gordon takes a day. At that point I pretty much stop caring about the issue since I feel neither does Gordon (which is a grave misjudgment as it turns out). I revisit the issue only one week later at which point I now can only assume Gordon had given up and in a last attempt to provide a direction suggest looking at FS errors, missing the question asked in Gordon&rsquo;s reply.</p>

<p>Gordon makes the mistake of not reading the documentation Mark gave him earlier or the question he posted next day would have already been answered and probably waiting a day with replying to the question.</p>

<h2>Act 5 conclusion</h2>

<p>I need to stop drawing conclusions so quickly and read bug requests more carefully or I would not have missed the migration question in the bug report. It would also be worth a try not to treat less urgent tickets with less care, it sucks to have tickets open the goal should be to close (as in resolve) then as soon as possible even if they seem not urgent — this applies especially to bugs.</p>

<p>Gordon could have made his own life a lot easier by including more information in the ticket, noting that it is an urgent issue, including the history of what he already did to debug would have both helped a lot. In addition to that actually reading the documentation Mark provided would have answered the migration question beforehand.</p>

<p>Mark while out of the picture already could have included the chat history in the ticket when seeing that Gordon did not — this admittedly is asking a lot.</p>

<h1>Act 0 - how everything started</h1>

<p>I know this is the wrong order, but Star Wars got away with it too. Now after the fact I know more then I did before. Reading Gordon&rsquo;s blog posts shed some light on the history and I think this is where actually things started to go wrong.</p>

<p>I am glad for every person choosing to try out FiFo, it means people put trust in what we&rsquo;ve build and that is a really cool thing. But please if you want to use something in production inform yourself ahead of time. Don&rsquo;t put yourself and your customers at risk by blindly running into things.</p>

<p>Talk to us, even before you start deploying! We know FiFo inside out, everyone in the FiFo team is running it themselves either for fun, for profit, for testing or for all three things. The channel is helpful too, there are more people outside the core team on the channel too who will gladly share their stories.</p>

<p>To put this straight, you&rsquo;ll not only get the software for free you will even get some &ldquo;consulting&rdquo; tossed in the mix for not anything more then just asking! That is of cause in a sensible limit and given we have time, but there is always half an hour to spare here or there.</p>

<h2>Act 0 conclusions</h2>

<p>Deploying in a single node for a production system was not the best move, FiFo clusters for a reason and a distributed setup has many advantages over a single node. Probably some of the config settings could have been tweaked for a better user experience. It might have even made sense to run on dev instead of release or at least to switch early.</p>

<p>Had we known the surrounding circumstances helping might have been a lot easier.</p>

<h1>Act 1 - be active in the community</h1>

<p>There is a huge advantage to this. And I don&rsquo;t even mean the fact that the community grows and everyone benefits. Being present in the channel and occasionally talking to people helps you to stay in the loop, know what is going on and what other people face for problems or find for solutions.</p>

<p>It also gives the benefit to influence the course of the project, a lot of the features were thought up and discussed within the community. And last but not least being active might actually end up in helping others, which will make the community as a while stronger.</p>

<h2>Act 1 conclusion</h2>

<p>I admit I am totally biased here. Everyones time is limited. When I have to chose to help a stranger I have no idea who it is or someone I know and has contributed in one way or another to the community I will pick the community member every time.</p>

<p>I can only talk for me here but I know for a fact that if Gordon or one of his colleges had been around in the channel and were a known face I would have taken the problem more serious. That said I have no idea if that is good or bad that I put community members before strangers.</p>

<h1>Act 2 - read the fantastic manual</h1>

<p>As Gordon points out FiFo not a trivial application. And he is entirely right, it is not, and there are may reasons for that which I am not going to argue here if that is a good or bad thing I will simply state that I have thought long about every choice I did in FiFo and claim them to be sound.</p>

<p>But we are well aware that it is not a simple pice of software like a editor or something, that is why we put a lot of time and effort in providing a manual and an extensive set of informations surrounding it.</p>

<p>We have a fully fledged manual, guides for Installation, Migration, and update. Best practice articles for Scaling, Networking and clustering. Checklists for problems and known issues. A list of terminology, information about our versioning system, recorded trainings (admittedly not much) and videos on usage.</p>

<p>For people interested in developing we have API documentations, starter guides, a documented build process. We have a list of internal libraries, and specifications on data structures. A guide to plugins, the messaging system and how to write plugins.</p>

<p>And last but not least we even have <a href="http://project-fifo.net/pages/viewpage.action?pageId=4194636">a page in which we explain how to best submit a bug</a></p>

<h2>Act 2 conclusion</h2>

<p>Reading those documents this would probably have saved a lot of time and pain and the fifo team some work. This is a reoccurring problem that we sadly see way too often  - if anyone has suggestions how to encourage users to read manuals please share the holy grail.</p>

<p>The documents would have given good advice on how to set up a redundant fifo, how to check for problems and perhaps most importantly how to properly report a bug.</p>

<h1>The Rant</h1>

<p>Given I spent the last two years of my life working on Project FiFo I feel I am entitled to this. Bad bug reports are a pet peeve! And this was a prime example of one.</p>

<p>All the right signs were there, a entirely nonsensical title the catch phrases &lsquo;it stopped working&rsquo; and &lsquo;nothing had changed&rsquo;. I can&rsquo;t say if this is the one in a million where that was actually true but in all my time in IT I have never seen those words to be correct, nor have I ever heard from someone else that saw the mystical situation of something just stopping to work.</p>

<p>There was no usable information in there, no sign of interest to actually help the process of finding the root cause. Just because FiFo is free and there is no charge in using it does not mean my time is worth any less then yours, gladly help you with a problem but I&rsquo;ll expect some engagement in return. I do not like to have my time wasted by having to pull every but of information out of someones nose.</p>

<p>Bottom line is: If you don&rsquo;t care enough about your problem to put some effort into getting help I will not care enough to help.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Backups With Project FiFo]]></title>
    <link href="http://blog.licenser.net/blog/2014/01/01/backups-with-project-fifo/"/>
    <updated>2014-01-01T12:15:37+01:00</updated>
    <id>http://blog.licenser.net/blog/2014/01/01/backups-with-project-fifo</id>
    <content type="html"><![CDATA[<p>With 0.4.3 FiFo introduces support for <a href="http://www.leofs.org">LeoFS</a> and this allows for some quite nice new features. Most importantly it decouples FiFo&rsquo;s operations from storing big amounts of data which makes maintaining either of this much more sensible and scaling storage much more easy.</p>

<p>Then again while nice that is not the important part, just storing datasets somewhere else does not make much of a difference for most users but what LoeFS allows FiFo to store much more data then would be good in the old setup. &lsquo;A lot more&rsquo; here means pretty much as much as you can store.</p>

<p>So with this options 0.4.3 FiFo introduce backups! Backups complement the snapshots already in the system for quiet a while but while snapshots were made to stay on the hypervisor backups are supposed to be shipped off to LeoFS. This not only helps to keep the number of snapshots limited, does not count against the local quota but also widens the failure domain.</p>

<p>And to make it better there is a sensible concept about incremental and full backups, that said there are a few limitations to be aware of:</p>

<ul>
<li>Backups that stay on the hypervisor will count against the local quota.</li>
<li>Once a backup is moved away from the hypervisor it can&rsquo;t be restored without overwriting the current state.</li>
<li>Restoring a backup might mean first deleting the local zfs volume for the vm.</li>
</ul>


<p>But that aside that there are some very interesting things:</p>

<ul>
<li>While it&rsquo;s not possible to keep multiple branches with snapshots this is very well possible with backups.</li>
<li>It&rsquo;s possible to make a difference between incremental and full backups choosing between recovery speed or space efficiency.</li>
</ul>


<p>All that sums up to something quite awesome, it allows for proper grandfather backups concepts for VM&rsquo;s and they can even be scripted using the fifo python client. So here is an example how this could be done.</p>

<p>Lets quickly describe what we want to achieve:</p>

<ul>
<li>Every month we want a full backup.</li>
<li>Every week we want an incremental backup

<ul>
<li>for the first week in a month towards the monthly full backup</li>
<li>for other weeks to the previous week</li>
</ul>
</li>
<li>Every day we want a incremental backup

<ul>
<li>for the first day of a week from the week backup</li>
<li>for other days from the previous day</li>
</ul>
</li>
</ul>


<p>To allow for the incremental backups we need to keep some of the backups around:</p>

<ul>
<li>monthly until the first weekly was done.</li>
<li>weekly until the next weekly was done but not longer the the next monthly.</li>
<li>daily until the next daily but not longer then the next weekly or monthly.</li>
</ul>


<p>The FiFo backup code fortunately helps a lot with this, a important part of the logic is <em>&lsquo;create a incremental snapshot and delete its parent from the hypervisor&rsquo;</em> and that is exactly the behavior of the backup code when passing both a parent and requesting a delete.</p>

<p>Here some <a href="https://github.com/project-fifo/pyfi/blob/master/examples/backup.sh">code</a> (with some comments added):</p>

<pre><code class="bash">#!/usr/bin/env bash
fifo=fifo
vm="$2"
case $1 in
    monthly)
        $fifo vms backups $vm create monthly
        # After createing a new monthly snapshot we first delete the last weekly and daily backup
        last_daily=$($fifo vms backups $vm list -pH --fmt uuid,local,comment | grep 'daily' | grep 'YES' | tail -1)
        if [ ! -z "$last_daily" ]
        then
            daily_uuid=$(echo $last_daily | cut -d: -f1)
            # the -l flag tells FiFo to only remove the backup from the hypervisor.
            $fifo vms backups $vm delete -l $daily_uuid
        fi
        last_weekly=$($fifo vms backups $vm list -pH --fmt uuid,local,comment | grep 'weekly' | grep 'YES' | tail -1)
        if [ ! -z "$last_weekly" ]
        then
            weekly_uuid=$(echo $last_weekly | cut -d: -f1)
            $fifo vms backups $vm delete -l $weekly_uuid
        fi
        ;;
    weekly)
        last_backup=$($fifo vms backups $vm list -pH --fmt uuid,local,comment | grep 'monthly\|weekly' | grep 'YES' | tail -1)
        uuid=$(echo $last_backup | cut -d: -f1)
        $fifo vms backups $vm create --parent $uuid -d weekly
        # After creating a new weekly we need to make sure to delete the last daily one
        last_daily=$($fifo vms backups $vm list -pH --fmt uuid,local,comment | grep 'daily' | grep 'YES' | tail -1)
        if [ ! -z "$last_daily" ]
        then
            daily_uuid=$(echo $last_daily | cut -d: -f1)
            $fifo vms backups $vm delete -l $daily_uuid
        fi
        ;;
    daily)
        last_backup=$($fifo vms backups $vm list -pH --fmt uuid,local,comment | grep 'daily\|weekly' | grep 'YES' | tail -1)
        uuid=$(echo $last_backup | cut -d: -f1)
        type=$(echo $last_backup | cut -d: -f2)
        case $type in
            weekly)
                $fifo vms backups $vm create --parent $uuid daily
                ;;
            daily)
                $fifo vms backups $vm create --parent $uuid -d daily
                ;;
        esac
        ;;
esac
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Asynchronously GCed or Set]]></title>
    <link href="http://blog.licenser.net/blog/2013/06/13/a-asynchronous-gced-or-set/"/>
    <updated>2013-06-13T11:09:00+02:00</updated>
    <id>http://blog.licenser.net/blog/2013/06/13/a-asynchronous-gced-or-set</id>
    <content type="html"><![CDATA[<p>Following the <a href="http://blog.licenser.net/blog/2013/06/11/asyncronous-garbage-collection-with-crdts/">article about Asynchronous garbage collection with CRDTs</a> I experimented with <a href="https://github.com/Licenser/ecrdt/blob/master/src/vorsetg.erl">implementing the concept</a>. The OR Set is a very nice data structure for this  since it&rsquo;s rather simple and so is it&rsquo;s garbage!</p>

<p>To garbage collect the <a href="https://github.com/Licenser/ecrdt/blob/master/src/vorset.erl">OR Set</a> we do the following, we take some of the elements of the remove set, and delete them from both the add and the remove set - this way we save the space for them and generate a new baseline.</p>

<p>First step was to implement the data structure described to hold the collectable items, I call it a <a href="https://github.com/Licenser/ecrdt/blob/master/src/rot.erl">ROT</a> (Roughly Ordered Tree) it&rsquo;s a nice name for garbage related stuff ;) and it is treeish and mostly ordered.</p>

<p>The interface of the ROT is rather simple, Elements must be time tagged, in the form {Time, Element}. Where time must not be a clock, as long as the Erlang comparison operations work on it to give an order. Then it allows asking for full buckets, and removing buckets based on their hash value and newest message timestamp.</p>

<p>While the elements in a the OR set area already tagged with a timestamp, this timestamp records addition, not deletion so it would be misleading to use them since the ROT would think the remove happened when actually the addition happened and this would violate the rule that no event can travel back behind T<sub>100</sub>. As a result we&rsquo;ll have to double timestamp the removes - as in add a second when when it was removed.</p>

<p>So since the ROT has a very similar interface to a G Set (which implemented the remove set before) the change is trivial. <strong>Remove</strong>, <strong>GC</strong> and the <strong>merge</strong> function are more interesting.</p>

<h3>remove</h3>

<pre><code class="erlang">remove(Id, Element, ORSet = #vorsetg{removes = Removes}) -&gt;
    CurrentExisting = [Elem || Elem = {_, E1} &lt;- raw_value(ORSet),
                               E1 =:= Element],
    Removes1 = lists:foldl(fun(R, Rs) -&gt;
                                   rot:add({Id, R}, Rs)
                           end, Removes, CurrentExisting),
    ORSet#vorsetg{removes = Removes1}.
</code></pre>

<p><code>Id</code> defaults to a the current time in nanoseconds since it&rsquo;s precise enough for most cases, but can be given any value that provides timed order. Line 2 and 3 collect all observed and not yet removed instances of the element to delete, we then fold over those instances and add each of them to the ROT.</p>

<h3>GC</h3>

<pre><code class="erlang">gc(HashID,
   #vorsetg{
      adds = Adds,
      removes = Removes,
      gced = GCed}) -&gt;
    {Values, Removes1} = rot:remove(HashID, Removes),
    Values1 = [V || {_, V} &lt;- Values],
    Values2 = ordsets:from_list(Values1),
    #vorsetg{adds = ordsets:subtract(Adds, Values2),
             gced = ordsets:add_element(HashID, GCed),
             removes = Removes1}.
</code></pre>

<p>To GC the set we take the <code>HashID</code>, this is what the rot returns when it reports full buckets, and in line 6 remove it from the ROT. Thankfully the ROT will return the content of the deleted bucket, this comes in very handy, since in the process of garbage collecting the bucket we also need to remove the items once and for all from the add list as seen in line 9. We then record the GC action in line 10 to make sure it will applied during a merge.</p>

<p>Please note that currently this set, even so it is garbage collected still grows without bounds since the GC actions themselves are not (yet) garbage collected, this will be added in a later iteration.</p>

<h3>merge</h3>

<pre><code class="erlang">merge(ROTA = #vorsetg{gced = GCedA},
      ROTB = #vorsetg{gced = GCedB}) -&gt;
    #vorsetg{
       adds = AddsA,
       gced = GCed,
       removes = RemovesA}
        = lists:foldl(fun gc/2, ROTA, GCedB),
    #vorsetg{
       adds = AddsB,
       removes = RemovesB}
        = lists:foldl(fun gc/2, ROTB, GCedA),
    ROT1 = rot:merge(RemovesA, RemovesB),
    #vorsetg{adds = ordsets:union(AddsA, AddsB),
             gced = GCed,
             removes = ROT1}.
</code></pre>

<p>Merging gets a bit more complicated due to the fact that we now have to take into account that values might be garbage collected in one set but not in the other. While merging them would do no harm it would recreate the garbage which isn&rsquo;t too nice. So what we do is applying the recorded GC actions to both sets first as seen in line 3 to 11 and then merge the remove values (line 12) finally the add values (line 13).</p>

<h2>Results</h2>

<p>I set up some proper tests for the implementation, comparing the GCed OR Set (bucket size 10) with a normal OR Set, running 1000 iterations with a set of 1000 instructions composed of 70% adds and removes, 20% merges and 10% GC events. T<sub>100</sub> is a sliding time from the allowed collection of events older then the last merge.</p>

<p>Each stored element had the size of between 500 and 600 bytes (so there were 100 possible elements). A remove will always remove the stalest element, since they are added in random order this equals a random remove.</p>

<p>The operations are carried out of replicas copies of the set where add, and remove have a equal chance to be either happening just on copy A, or just on copy B, or on both replicas at the some time. GC operations are always carried out on both replicas but it should be noted that the GC operation does not include a merge operation so can be considered asynchronous.</p>

<p>All operations but the GC operation are executed exactly the same on the GCed OR set and the not GCed or Set in the same order and same spread.</p>

<p>At the end a final merge was performed and the resulting values compared for each iteration, no additional GC action takes place at the end.</p>

<p>Measured were both the space reduction per GC run and the final difference of size. Per GC run about 15% space was reclaimed and at the end the GCed set had a total space consumption of around 26% of the normal OR Set in average, 6% in the best and 143% in the worst case.</p>

<pre><code>src/vorsetg.erl:389:&lt;0.135.0&gt;: [Size] Cnt: 1000,   Avg: 0.261,  Min: 0.062, Max: 1.507
src/vorsetg.erl:389:&lt;0.135.0&gt;: [ RS ] Cnt: 49221,  Avg: 0.866,  Min: 0.064, Max: 1.0
src/vorsetg.erl:389:&lt;0.135.0&gt;: [ GC ] Cnt: 49221,  Avg: 55.870, Min: 0,     Max: 6483
src/vorsetg.erl:389:&lt;0.135.0&gt;: [ MG ] Cnt: 98357,  Avg: 58.110, Min: 0,     Max: 6596
src/vorsetg.erl:389:&lt;0.135.0&gt;: [ OP ] Cnt: 344708, Avg: 38.539, Min: 0,     Max: 6916```
</code></pre>

<p>The numbers are from a test run, for readability truncated manually after 3 digest and aligned to be nicer readable. <strong>Size</strong> is total size at the end of the iteration, <strong>RS</strong> is the space reduction per GC run. <strong>GC</strong>, <strong>MG</strong> and <strong>OP</strong> are the time used for garbage collection, merging and other operations respectively, the numbers are per execution and measured microseconds. Time measurements also include noise that from additional operations required for the test and should not be seen as a useful benchmark!</p>

<h2>Conclusion</h2>

<p>The GC method described seems to work, and not even too badly, in the course of experimenting with values it showed that the conserved space is heavily dependant on the environment like the bucket size chosen, the size of the elements, the add/remove ratio and the ratio on which merges happen.</p>

<p>The OR Set it was compared with was not optimised at all, but thanks to it&rsquo;s simplicity a rather good candidate, the gains on already optimised sets will likely be lower. (run with a <a href="https://github.com/Licenser/ecrdt/blob/master/src/vorset2.erl">optimised OR Set</a> gave only 1 54% reduction in space instead of a 74% one with a normal OR Set).</p>

<p>The downside is that garbage collection takes time, so does merging, so a structure like this is over all slower then a not garbage collected version</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Asynchronous Garbage Collection With CRDTs]]></title>
    <link href="http://blog.licenser.net/blog/2013/06/11/asyncronous-garbage-collection-with-crdts/"/>
    <updated>2013-06-11T15:45:00+02:00</updated>
    <id>http://blog.licenser.net/blog/2013/06/11/asyncronous-garbage-collection-with-crdts</id>
    <content type="html"><![CDATA[<p>So CRDTs are very very nice data structures awesome for eventual consistent applications like riak, or the components of <a href="http://project-fifo.net">Project-FiFo</a>. So they have one big drawback, most of them collect garbage, and over time that can sum up to a lot making them pretty unpractical in many cases. Collecting this garbage is a bit tricky, since usually it means synchronising the data - which going back to the eventual consistent stuff prevents either A or P.</p>

<p>I want to outline some thoughts here how one could deal with this issue. As usual the idea here isn&rsquo;t without tradeoffs, it does impose certain constrains on the systems behaviour and does not fit every behaviour in exchange of allowing garbage to be disposed of without the need of synchronisation. Now then, lets dive right in.</p>

<h2>What&rsquo;s that trash?</h2>

<p>We start with understanding what the garbage is that sums up. To allow CRDTs to work the way we do, they need to store some kind of history or legend of how the current state (version/value) of the CRDT came to existence.</p>

<p>If we look at a OR Set for example the history of this set is stored by recording all elements ever added along with all elements ever deleted - elements are tagged to be unique too so adding 5, removing 5 and adding 5 again and removing that again, leaves not a data structure with 0 elements but one with 4. That said there are ways to optimise the OR Set bot lets ignore this for the sake of the example. We can&rsquo;t just store an empty list since we need to make sure that when another copy of the same set can recreate the steps even if it just missed one of the events.</p>

<p>Actually we could, if we would synchronise all copies, say hey ¯from now on you all agree that the new <strong>baseline</strong> (this is bold since it will come up a few more times) is an empty set from now on. And  doing that we would have garbage collected the OR Set, disposed of data that isn&rsquo;t directly relevant to the current state any more.</p>

<p>If we don&rsquo;t guarantee that all objects are garbage collected to the same state, we face a real issue, since the new baseline will cause quite some trouble since the partially applied effects will just be applied again and possibly cause them to be doubly applied. <strong>Or in short, partially applied GCing will cause the CRDT to stop functioning.</strong></p>

<h2>Things get old.</h2>

<p>Looking at the data that gathers and how it is distributed there is one observation to be made: the older a change in state is the more likely it is to be present in all replicas. It makes sense, with eventual consistency we say &lsquo;eventually&rsquo; our data will be the same everywhere, and the chances of &lsquo;eventual&rsquo; are growing the older the change is since it will get more chance to replicate. (mechanisms similar to riak&rsquo;s AAE greatly help here).</p>

<p><img src="http://blog.licenser.net/images/posts/2013-06-11-asynchronous-garbage-collection-with-crdts1.png" alt="state distribution" /></p>

<p>So generally there is a T<sub>100</sub> from which point on older data is shared between all instances and by that no longer relevant if we could just garbage collect it. But we don&rsquo;t want synchronous operations, nor do we want partial garbage collection (since that rally would suck).</p>

<p>Back to state, we know which ones we want to garbage collect, lets say we record not only the state change but a timestamp, a simple non monotonic system timestamp, it&rsquo;s cheap to get. Keep in  mind T<sub>100</sub> is well in the past, so if the precision of the times taps is good enough to guarantee that a event at T<sub>0</sub> can not travel back behind T<sub>100</sub>, it&rsquo;s OK if order between T<sub>0</sub> and T<sub>99</sub> changes all the time, we don&rsquo;t really care about that so lets store the state data in a way that helps us with this:</p>

<p>T<sub>0</sub> [S<sub>0</sub>,S<sub>1</sub>, &hellip;, S<sub>n</sub>] T<sub>100</sub> [S<sub>n+1</sub>, &hellip;, S<sub>n+m</sub>]</p>

<h2>A trash bin</h2>

<p>But since it would really suck (I know I&rsquo;m repeating myself) if we partially GC the data we want to be sure that we agree, so would could go and ask all the replicas for their old data (older then T<sub>100</sub>). Yet this approach has a problem, for once T<sub>100</sub> will shift in the time we check, then this might be more data to move then we care for.</p>

<p>So lets use a trash bin, or multiple once order our data in them so you&rsquo;ve some groups of old messages, bunched together which can be agreed on, no matter on the time moving and they are smaller portions. Something like this</p>

<p>&hellip; T<sub>100</sub> [S<sub>n+1</sub>, &hellip;, S<sub>n+100</sub>] [S<sub>n+101</sub>, &hellip;, S<sub>n+200</sub>]&hellip;</p>

<p>So we just have to agree on some bucket to garbage collect, since so if there is another half full bucket now since T<sub>100</sub> has moved since the agreement we don&rsquo;t really care about that. Thanks to the fact that operations are commutative we also can garbage collect in a non direct order, so it&rsquo;s not a biggie if we take just one bucket and not the oldest one.</p>

<p>We&rsquo;re still left with transmitting (in this example) 100 elements to delete and haven&rsquo;t solve the problem of partial garbage collection, but at least we&rsquo;re a good step closer, we&rsquo;ve put the garbage in bins now that are much easier to handle then just on a huge pile.</p>

<h2>A garbage compactor</h2>

<p>Lets tackle the last two issues we do a little trick, instead of sending out the entire bucket we compress it, create a hash of it and send this back and forth, so instead of:</p>

<p>[S<sub>n+1</sub>, &hellip;, S<sub>n+100</sub>]</p>

<p>We tag this bucket with a hash (over it&rsquo;s content) and the newest timestamp of the first element. Since it&rsquo;s older then T<sub>100</sub> we do not need to worry of it changing and recreating the hash, and we get something like this:</p>

<p>(hash, T<sub>S<sub>n+1</sub></sub>)[S<sub>n+1</sub>, &hellip;, S<sub>n+100</sub>]</p>

<p>To agree on buckets to collect and to give the collect order we just need to send the hash and timestamp and an identifier, this is pretty little data to send forth and back. This solves the send much data problem, curiously it also helps a lot with the partial garbage collection status.</p>

<h2>A schedule for garbage collection</h2>

<p>With only the buckets tag identifying it we can solve the partial collection issue, we just treat garbage collection as just another event, storing it and replaying it if it wasn&rsquo;t present in a old replica. So we gradually progress the baseline of a replica towards the common baseline somewhat like this:</p>

<p><img src="http://blog.licenser.net/images/posts/2013-06-11-asyncronous-garbage-collection-with-crdts2.png" alt="gc graph" /></p>

<p>Ideally we store the GC operations in a own list and since we can easier apply it then and guarantee that the GC events are synchronised and applied before other events.</p>

<p>That&rsquo;s it, and should be a somewhat working implementation of asynchronous garbage collection for CRTDs. But it&rsquo;s not perfect so lets take a look at the downsides before we end this.</p>

<h2>Lets be honest, it still has a downside</h2>

<p>This concept of GCing does not come for free, the data structure required isn&rsquo;t entirely trivial so it will add overhead, even so the current <a href="https://github.com/Licenser/ecrdt/blob/master/src/rot.erl">implementation</a> is pretty cheap when adding the events in right order, wrong order will cause additional overhead because it might cause elements to shift around in the structure.</p>

<p>It requires events to be timestamped, even so there is no requirement for absolute order, this adds a constraint to messages and events that wasn&rsquo;t there before. Also this is additional work and space that is consumed.</p>

<p>We need to define a T<sub>100</sub> for the system and guarantee it, and find a balance of choosing a big enough T<sub>100</sub> to ensure it&rsquo;s correctness while keeping it small enough to not keep a huge tail of non garbage collected events. That said this can be mitigated slightly by using a dynamic T<sub>100</sub> for example put record when a object was last written to all primary nodes.</p>

<p>If T<sub>100</sub> isn&rsquo;t chooses correctly it might end up getting really messy! if a elements slips by T<sub>100</sub> that wasn&rsquo;t there it could mean that the garbage collection is broken for quite some while or worst state gets inconsistent.</p>

<p>Bucket size is another matter, it needs to be chosen carefully to be big enough to not spam the system but small enough to not take ages to fill, a event passing T<sub>100</sub> but not filling the bucket isn&rsquo;t doing much good.</p>

<p>This is just a crazy idea. I haven&rsquo;t tried this, implemented it or have a formal prove, it is based on common sense and my understanding on matters so it might just explode ;)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Happy Birthday Project FiFo]]></title>
    <link href="http://blog.licenser.net/blog/2013/05/06/happy-birthday-project-fifo/"/>
    <updated>2013-05-06T00:00:00+02:00</updated>
    <id>http://blog.licenser.net/blog/2013/05/06/happy-birthday-project-fifo</id>
    <content type="html"><![CDATA[<p>Some might know it, some might not and some might not care but for what it&rsquo;s worth I&rsquo;m the author of <a href="http://project-fifo.net">Project-FiFo</a> (or most of it) and today is Project-FiFo&rsquo;s first birthday (since the domain registration) and I want to take this chance to look back to the past year and reflect, say thank you to all of you and take a look in the future.</p>

<p>When I started Project FiFo a year ago it was more of a tiny hobby project and I could have sworn it would stand in row with all the other little open source projects no one would ever give a damn about. I really could not have been more wrong, what started as a <a href="https://github.com/project-fifo/vmwebadm">few lines of clojurescript</a> has grown to a beat of project with thousands of lines of code, a ever growing and incredible community (the project page gets between 2.5 and 3 thousand visitors a month by now and constantly more then 20 people in the irc channel) and a totally enthusiastic team!</p>

<h2>Thank you</h2>

<p>With a year gone it is about time to call out a few people and say &lsquo;thanks&rsquo; because without their time, effort and work FiFo would not exist and in the day to day business of killing bugs, adding features and contemplating world domination it&rsquo;s easy to forget this.</p>

<p>I want to start with <a href="http://blog.smartcore.net.au/">Mark Slatem aka trentster</a> author of the SmartCore blog and FiFo&rsquo;s <a href="http://ukiahman.tripod.com/Ryker_7.jpg">number one</a>. He was pretty much the first person looking into FiFo and has sticked around till now going from first observer to tester, <a href="http://project-fifo.net/display/PF/The+FiFo+Manual">writer</a>, helper and most of all a good friend.</p>

<p><a href="http://www.beginningwithi.com/">Deirdré</a> the Joyent community manager for SmartOS. Solaris and with that SmartOS is a underdog, and I&rsquo;m sure without the incredible brilliant community it would have been doomed from the start. But it is not, and in a good part thanks to Deirdrés effort to shape the community and make it part of the ecosystem instead of a second class citizen.</p>

<p>Killphil author of jingles, FiFo&rsquo;s web UI, it is amazing he popped up one day, out of the blue saying &lsquo;hey I&rsquo;ve played a bit with improving your UI&rsquo; and put jingles down with became the official UI within matters of days. Sadly I could not get rid of him again since then so you all have to live with him adding <a href="https://monosnap.com/image/QNxrmvEecVoly173dt5OOrVgc">crazy new features</a>.</p>

<p><a href="http://joyent.com">Joyent</a> as a whole for open sourcing SmartOS and making it better and better. Without SmartOs there sure would be no FiFo and I&rsquo;d be stuck with Linux/KVM, which would be not too much fun.</p>

<p><a href="http://basho.com">basho</a> who open sourced riak_core and riak_test which make fifo so much more incredible and provide me with free T-Shirts (I got four by now but please don&rsquo;t tell them or I might not get any more).</p>

<p>Every single person using FiFo, it&rsquo;s amazing to see how well the project is received, hear the feedback. Thanks a lot for all the help with the little things, for putting up with the occasional bugs and bearing with the time it might take to fix them or add new features.</p>

<h2>Looking back</h2>

<p>I&rsquo;ve been working on FiFo for a year now, well first attempts counted a bit longer even, and without exaggerating this was the most amazing year in my life. It has been a blast, I&rsquo;ve learned tons of things, both technical and socially, meet some of the most amazing people I can think of and honestly never have been this happy before.</p>

<p>The whole thing started when I wanted to share a co-located server with some friends who are kind of consoleophobe and I wasn&rsquo;t happy with the approach to give everyone root access. So a solution had to be found but vanilla SmartOS provided none, SDC was making no sense with a single node and too expensive for a hobby system. Everything else was simply not Solaris, period. Adding to it that Deirdré showed me the community, randomly answering a question on twitter with a hint to visit the channel - which was was incredible surprising after experiencing some of the Linux community… really there was no chance in hell ending up with anything but SmartOS.</p>

<p>But all in all there was no virtualisation solution that suited what I wanted, not even if I had taken cost out of the equation. And since I refuse to swing the white flag and surrender to something I don&rsquo;t like the only wan was: build one! (also I&rsquo;m crazy about challenges and seeing how far I can push things ;) And after</p>

<p>That sums up how the whole thing started, with a little nodejs/clojurescript application that could be used with sdc-* commands over http. But that did only work with a single host, not that I had more to serve but it looked kind of clumsy and unprofessional for a cloud operating system like SmartOS so wiggle was born as kind of a broker in front of multiple vmwebadm (man the name was horribly boring). And from there on it kept growing and growing.</p>

<p>Now over the last year of work, and lots of lots of input from the community the one badly named program has become 5 services, three of them distributed via riak_core, and a HTML/JS UI on top of that, that most importantly, all have very cool names.</p>

<p>All the technical things aside, running an open source project where people get engaged is a fascinating experience, there is so much to learn I would have never dreamed of, so much to take away from the situation that helped me understand the problems and inner working of teams, people, projects
better. That alone was worth every second of time invested.</p>

<h2>Dogs and funky names</h2>

<p>Now before we go on I share something that was asked a few times now: why the crazy names and obsession with dogs?</p>

<p>So the story starts with naming the first component, which back then was wiggle (after being very disappointed with my name choice for vmwebadm). Wiggle was the component that gave a unified interface to multiple hypervisors and in the SmartOS channel I had heard that Joyent called it&rsquo;s thing &lsquo;headnode&rsquo;, but for FiFo the goal was never to clone something that already existed and I wanted to make a point of it so here is how the thought chain went: head -> tail, node -> nod -> wiggle, tail&amp;wiggle -> dog.</p>

<p>Now I had a promise to keep, back when I was younger and my brother was even younger (since he is my little brother) he got a pet (as in not real) dog, and I had just learned with a FiFo (First in First out) queue is, I found that Fifo is a amazing name for a dog, so I talked my brother into naming his pet dog Fifo telling hime that if I ever had a dog I&rsquo;d name it Fifo too, that said, I&rsquo;m a man of my word even if it takes over something like 15 years to make good on it.</p>

<p>All other names just followed the same naming scheme, expect jingles but then again I did not name it myself. That said Fifo, the dog, is still with us, he is sitting on my window board drying from taking a shower earlier today to get cleaned up for it&rsquo;s birthday!</p>

<p>As a nice side effect it is great for people to remember things that are named so silly as FiFo&rsquo;s components!</p>

<h2>Looking ahead</h2>

<p>To be honest I feel that even after a year FiFo is still in its infancy, don&rsquo;t get me wrong it&rsquo;s quite stable and the features it provides build a very good foundation but there is so much more it can and will be!</p>

<p>PXE booting, integrated in FiFo, allowing to spin up new hypervisors by a click in the UI (or a call from he console) adding ipmi to the mix makes it even more exciting! Think about automatically booting a new hypervisor when the capacity reaches a certain percentage, or shutting an empty one down when it&rsquo;s below.</p>

<p>Support for Clouds spread over WAN, location awareness of VM&rsquo;s and Hypervisors with a notion of distance with deployment rules that take this into account (please don&rsquo;t deploy all my database cluster VM&rsquo;s on the same physical host, but don&rsquo;t spread them over multiple datacenters!).</p>

<p>Cold migration of VM&rsquo;s from one host to another, and putting them into cold storage / backing them up as a whole.</p>

<p>Well, I could go on and on rambling about crazy ideas for another thousand or so words or so but lets save this for another time. All in all I wanted to say it was an amazing year, amazing to see the community develop, seeing how FiFo gets used and I am hugely excited to see how things continue from here on! I can&rsquo;t wait to see the first 10+ node FiFo setup, hear what people make out of it. See a first adopted UI, people starting to build things around FiFo - there already is a <a href="https://github.com/bakins/project-fifo-ruby">ruby implementation</a> of the API along with a <a href="https://github.com/bakins/knife-fifo">chef knife thing</a>.</p>

<p>So to close: Happy birthday FiFo, thanks to all of you for joining this journey and lets brace for another year of dog-named-components!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FiFo + 80LOC of Bash = 5 Node Riak Cluster]]></title>
    <link href="http://blog.licenser.net/blog/2013/04/23/fifo-plus-80loc-of-bash-equals-5-node-riak-cluster/"/>
    <updated>2013-04-23T14:47:00+02:00</updated>
    <id>http://blog.licenser.net/blog/2013/04/23/fifo-plus-80loc-of-bash-equals-5-node-riak-cluster</id>
    <content type="html"><![CDATA[<h2>The reason</h2>

<p>The question &lsquo;why would I want at least 5 nodes&rsquo; comes up very often in the #riak IRC channel, there is a <a href="http://basho.com/why-your-riak-cluster-should-have-at-least-five-nodes/">good explanation</a>. But that&rsquo;s boring, no one likes reading manuals, we, as engineers, like to try things out (aka. break stuff).</p>

<p>Only downside with that is that we need to set things up before we can break them, or even worst need to un-break it later to try out different things (aka. break it in different ways). Admittedly setting up a riak instance is <strong>easy</strong> but setting up 5 and connecting them then break them and do all again to break them again, erm… I mean try things out of cause, can get really tedious and I for once am too lazy to bother with that.</p>

<h2>The goal</h2>

<p>Make setting our breakage, erm test, bed setup as simple as possible, and whipping up things and tearing them down trivial, ideally have one simple command like <code>./riak.sh setup</code> to do that for us and <code>./riak.sh delete</code> undo it all for us to get back to a clean state.</p>

<h2>The tools</h2>

<p>To build anything we&rsquo;ll need some tools, hammer and nails will not do us much good here so we are going to pick:</p>

<ul>
<li><a href="http://project-fifo.net">Project FiFo</a> - my favourite virtualisation tool (I am biassed I wrote it), but it&rsquo;s very easy to set up and very powerful.</li>
<li><a href="https://github.com/project-fifo/pyfi">The FiFo Console Client</a> - we want to script things, a UI isn&rsquo;t helpful.</li>
<li>bash - the simplest possible scripting tool.</li>
<li>curl - since riak offers a http fronted it&rsquo;s a wonderful way to check if the system is up.</li>
<li><a href="http://trentm.com/json/">jsontool</a> - a nifty utility to traverse JSON documents.</li>
</ul>


<p>With that we should be set and good to go.</p>

<h2>The steps</h2>

<p>We&rsquo;ll have to perform multiple steps to build our wracking ground for riak lets look at them one by one:</p>

<h3>Preparing the environment</h3>

<p>Before we can begin we&rsquo;ve to set up a few things, I&rsquo;ll not go into detail how to set up FiFo, there is a [good manual] for that with only like 5 steps required. So lets start at some of the script&rsquo;s variables:</p>

<pre><code class="bash">#/usr/bin/env bash
PACKAGE="small"
DATASET="base64-1.9.1"
NET="7df94bc3-6a9f-4c88-8f80-7a8f4086b79d"
</code></pre>

<ul>
<li><code>smal</code> is the name of the package created in FiFo, I picked something with 512MB of memory since that should be enough for now.</li>
<li><code>base64-1.9.1</code> is the dataset it means things are running in a solaris zone this also can be installed from the FiFo UI.</li>
<li><code>7df94bc3-6a9f-4c88-8f80-7a8f4086b79d</code> is the UUID of the network you can find that with <code>fifo networks list</code></li>
</ul>


<pre><code class="bash">schroedinger:fifopy heinz [master] $ fifo packages list
                                UUID Name       RAM        CPU cap    Quota
------------------------------------ ---------- ---------- ---------- ----------
5f9f6c41-d700-4b4f-80f1-7350a71ed2e6 small      512 MB     100%       10 GB
schroedinger:fifopy heinz [master] $ fifo networks list
                                UUID Name       Tag                  First            Last
------------------------------------ ---------- ---------- --------------- ---------------
7df94bc3-6a9f-4c88-8f80-7a8f4086b79d test       admin        192.168.0.210   192.168.0.220
schroedinger:fifopy heinz [master] $ fifo datasets list
                                UUID Name       Version Type  Description
------------------------------------ ---------- ------- ----- ----------
60ed3a3e-92c7-11e2-ba4a-9b6d5feaa0c4 base       1.9.1   zone  A SmartOS ...
</code></pre>

<h3>Creating a VM with riak installed</h3>

<p>Creating a VM is rather simple we need a little JSON and pipe it to fifo with a cat. Please note the section reading <code>user-script</code> here we make the setup. Here is how it looks.</p>

<pre><code class="bash">cat &lt;&lt;EOF | fifo vms create -p $PACKAGE -d $DATASET
{
  "alias": "riak1",
  "networks": {"net0": "$NET"},
  "metadata": {"user-script": "/opt/local/bin/sed -i.bak \\"s/pkgsrc/pkgsrc-eu-ams/\\" /opt/local/etc/pkgin/repositories.conf; /opt/local/bin/pkgin update; /opt/local/bin/pkgin -y install riak; export IP=\`ifconfig net0 | head -n 2 | tail -n 1 | awk '{print \$2}'\`; /opt/local/bin/sed -i.bak \\"s/127.0.0.1/\$IP/\\" /opt/local/etc/riak/app.config; /opt/local/bin/sed -i.bak \\"s/127.0.0.1/\$IP/\\" /opt/local/etc/riak/vm.args; svcadm enable epmd riak"}
}
EOF
</code></pre>

<p>To get a bit better look user script section and remove the escape things:</p>

<pre><code class="bash"># We configure pkgin to use the european mirror you might not need to do that.
/opt/local/bin/sed -i.bak "s/pkgsrc/pkgsrc-eu-ams/" /opt/local/etc/pkgin/repositories.conf;
# We update the pkgin database and install riak
/opt/local/bin/pkgin update;
/opt/local/bin/pkgin -y install riak;
# We find out what IP our VM has from within the VM.
export IP=`ifconfig net0 | head -n 2 | tail -n 1 | awk '{print $2}'`;
# We update the app.config and vm.args to use the 'public' ip instead of the 127.0.0.1
/opt/local/bin/sed -i.bak "s/127.0.0.1/$IP/" /opt/local/etc/riak/app.config;
/opt/local/bin/sed -i.bak "s/127.0.0.1/$IP/" /opt/local/etc/riak/vm.args;
# Start epmd and riak
svcadm enable epmd riak
</code></pre>

<h3>Waiting for riak</h3>

<p>Now that is the first zone set up next we&rsquo;ll want to wait for riak to properly start up. This is needed since the commands are asynchronous and installing the packages can be a tad slow. But we can just to curl the http interface to check for this, so it&rsquo;s rather simple:</p>

<pre><code class="bash"># We'll ask fifo for the IP of our first zone.
IP1=`fifo vms get riak1 | json networks[0].ip`
# Print some info so waiting is not so boring
echo -n 'Waiting until riak is up and running on the primary node.'
# now we curl the http interface every second to see if things are good.
until curl http://${IP1}:8098 2&gt;/dev/null &gt;/dev/null
do
    sleep 1
    echo -n '.'
done
# and we're done!
echo " done."
</code></pre>

<h3>Setting up the remaining zones</h3>

<p>We&rsquo;re not going to get into too much details with this since it is pretty much working the same as the first VM with the only difference that the user-script holds two more lines:</p>

<pre><code class="bash">for i in 2 3 4 5
do
    cat &lt;&lt;EOF | fifo vms create -p $PACKAGE -d $DATASET
  {
    "alias": "riak${i}",
    "networks": {"net0": "$NET"},
    "metadata": {"user-script": "/opt/local/bin/sed -i.bak \\"s/pkgsrc/pkgsrc-eu-ams/\\" /opt/local/etc/pkgin/repositories.conf; /opt/local/bin/pkgin update; /opt/local/bin/pkgin -y install riak; export IP=\`ifconfig net0 | head -n 2 | tail -n 1 | awk '{print \$2}'\`; /opt/local/bin/sed -i.bak \\"s/127.0.0.1/\$IP/\\" /opt/local/etc/riak/app.config; /opt/local/bin/sed -i.bak \\"s/127.0.0.1/\$IP/\\" /opt/local/etc/riak/vm.args; svcadm enable epmd riak; sleep 10; /opt/local/bin/sudo -uriak /opt/local/sbin/riak-admin cluster join riak@${IP1}; /opt/local/bin/sudo -uriak /opt/local/sbin/riak-admin cluster plan; /opt/local/bin/sudo -uriak /opt/local/sbin/riak-admin cluster commit"}
  }
EOF
    IP=`fifo vms get riak$i | json networks[0].ip`
    echo -n "Waiting untill riak is up and running on the node $i."
    until curl http://${IP}:8098 2&gt;/dev/null &gt;/dev/null
    do
        sleep 1
        echo -n '.'
    done
    echo " done."

done
</code></pre>

<p>The two new lines are joining the node to the existing riak node which is quite easy, we can use $IP1 we generated in the first step too:
<code>bash
/opt/local/bin/sudo -uriak /opt/local/sbin/riak-admin cluster join riak@${IP1}
/opt/local/bin/sudo -uriak /opt/local/sbin/riak-admin cluster plan
/opt/local/bin/sudo -uriak /opt/local/sbin/riak-admin cluster commit
</code></p>

<p>This is run that up and you&rsquo;ve a 5 node riak cluster, and it&rsquo;s quick at last if you&rsquo;re in the US and have a good connection to the package repository.</p>

<p><a href="https://raw.github.com/project-fifo/pyfi/master/examples/riak.sh">Here</a> is this all slapped together.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Writing Your First Riak Test Test (Yes I Know There Are Two Tests There)]]></title>
    <link href="http://blog.licenser.net/blog/2013/04/09/writing-your-first-riak-test-test-yes-i-know-there-are-two-tests-there/"/>
    <updated>2013-04-09T23:33:00+02:00</updated>
    <id>http://blog.licenser.net/blog/2013/04/09/writing-your-first-riak-test-test-yes-i-know-there-are-two-tests-there</id>
    <content type="html"><![CDATA[<p>As promised in a <a href="http://blog.licenser.net/blog/2013/03/31/getting-started-with-riak-test-and-riak-core/">previous post</a> I&rsquo;ll talk a bit about writing tests for <code>riak_test</code>. To start with the obvious it&rsquo;s pretty simple and pretty awesome. <code>riak_test</code> gives you the tools you&rsquo;ve dreamed of when testing distributed <code>riak_core</code> applications:</p>

<ul>
<li>a backchannel to communicate and execute commands on the nodes.</li>
<li>a nice and way to bring up and tear down the test environment.</li>
<li>helper functions to deal with the <code>riak_core</code> cluster.</li>
<li>something called <code>intercepts</code> that allow you to mock certain behaviours in the cluster.</li>
<li>all the power of Erlang.</li>
</ul>


<h2>How a test works</h2>

<p>Tests have a very simple structre they pretty much contain a single function: <code>confirm/0</code></p>

<p>This function gets called when the test is executed and should return <code>pass</code> when everything works well or throw an exception when not. The actual test are simple unit asserts you use.</p>

<p>That in itself is not really overly exciting and those of you with a short attention span might start to thing &lsquo;boooooooring&rsquo; so lets look at the exciting part.</p>

<h2>Starting your applicatio</h2>

<p><code>riak_test</code> offers a way to start instances of your application and communicate with them, the common pattern is to start one (or more) nodes as first part of the script and check of they are up and running. That could look something like this:</p>

<pre><code class="erlang">confirm() -&gt;
    [Node] = rt:deploy_nodes(1),
    ?assertEqual(ok, rt:wait_until_nodes_ready([Node])),
    pass.
</code></pre>

<p>This is a minimal test it sets up one instance of our application and waits for it to be ready.</p>

<p><code>rt:deploy_nodes(1)</code> will deploy one node, the id of the node (a atom with that identifies it to erlang) will be sotred in <code>Node</code>, you can deploy more nodes by increasing the number in <code>rt:deploy_nodes/1</code>.</p>

<p><code>?assertEqual(ok, rt:wait_until_nodes_ready([Node]))</code> will make the test wait for the nodes to be ready, ready here means that all ring services we defined in the config are provided.</p>

<p>The node will be running in its own Erlang VM and have the test suite connected as a hidden node. This is the first thing that is truly interesting, since the connection will allow us to run rpc calls on the node this is the first thing that is truly fun.</p>

<h2>An official channel</h2>

<p>Now we&rsquo;ve a basic test running have our nodes to be started up and the test waiting until all is started and happy.</p>

<p>So chances are that aside of this back channel communication the node provides some kind of API, and we want to be able to connect to this API to run our tests. Un my case it&rsquo;s a simple TCP port that announces itself over mdns, we could simply listen to the broadcast and use the information it provides to talk to the node. This would work, as long as we&rsquo;ve a single node, the moment we&rsquo;ve to we would never know which node we&rsquo;re talking to and that would make testing hard.</p>

<p>So backchannel to the rescue! We&rsquo;ll just get the nodes configuration from the host, for my application I store this kind of information in the application configuration so I&rsquo;ve made a function that given a <code>Node</code> returns <code>IP</code> and <code>Port</code> for it to talk to plus one to send data:</p>

<pre><code class="erlang">node_endpoing(Node) -&gt;
    {ok, IP} = rpc:call(Node, application, get_env, [mdns_server_lib, ip]),
    {ok, Port} = rpc:call(Node, application, get_env, [mdns_server_lib, port]),
    {IP, Port}.

call(Node, Msg) -&gt;
    {IP, Port} = node_endpoing(Node),
    lager:debug("~s:~p &lt;- ~p", [IP, Port, Msg]),
    {ok, Socket} = gen_tcp:connect(IP, Port, [binary, {active,false}, {packet,4}], 100),
    ok = gen_tcp:send(Socket, term_to_binary(Msg)),
    {ok, Repl} = gen_tcp:recv(Socket, 0),
    {reply, Res} = binary_to_term(Repl),
    lager:debug("~s:~p -&gt; ~p", [IP, Port, Res]),
    gen_tcp:close(Socket),
    Res.
</code></pre>

<p>There is some <code>gen_tcp</code> stuff in there but lets just ignore it for now it&rsquo;s detail not important, the first function is the more interesting one it uses the <code>rpc</code> module to execute the commands on the node we just started which is quite awesome.</p>

<p>As a note I&rsquo;ve put those functions into <code>rt_&lt;applicatin name&gt;</code> so for example rt_sniffle.</p>

<h2>Testing the API</h2>

<p>With <code>call/2</code> we&rsquo;ve now a way to send data directly to the node over the official API channel so lets add to our test. So lets get back to our <code>confirm/0</code> and add some real tests:</p>

<pre><code class="erlang">confirm() -&gt;
    [Node] = rt:deploy_nodes(1),
    ?assertEqual(ok, rt:wait_until_nodes_ready([Node])),
    ?assertEqual({ok,[]},
                 rt_sniffle:call(Node, {vm, list})),
    ?assertEqual(ok,
                 rt_sniffle:call(Node, {vm, register, &lt;&lt;"vmid"&gt;&gt;, &lt;&lt;"hypervisor"&gt;&gt;})),
    ?assertEqual({ok,[&lt;&lt;"vmid"&gt;&gt;]},
                 rt_sniffle:call(Node, {vm, list})),                 
    pass.
</code></pre>

<p>It&rsquo;s as easy as this, this test will</p>

<ul>
<li>List the registered VM&rsquo;s and expect none to be there.</li>
<li>Create a new VM and expect a ok result.</li>
<li>List the registered vm&rsquo;s and expect the one we just registered to be present.</li>
</ul>


<p>And that&rsquo;s it for basic testing with <code>riak_test</code> I&rsquo;ll follow up on this with an article over intercepts since they add another cool feature to <code>riak_test</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Riak_test and Riak_core]]></title>
    <link href="http://blog.licenser.net/blog/2013/03/31/getting-started-with-riak-test-and-riak-core/"/>
    <updated>2013-03-31T07:30:00+02:00</updated>
    <id>http://blog.licenser.net/blog/2013/03/31/getting-started-with-riak-test-and-riak-core</id>
    <content type="html"><![CDATA[<p>If you don&rsquo;t know what <code>riak_core</code> is, or don&rsquo;t have a <code>riak_core</code> based application you&rsquo;ll probably not take too much practical use of this posts you might want to start with <a href="https://github.com/rzezeski/try-try-try">Ryan Zezeski&rsquo;s &ldquo;working&rdquo; blog try try try</a> and the <a href="https://github.com/basho/rebar_riak_core">rebar plugin</a>.</p>

<p>That said if you have a <code>riak_core</code> app this posts should get you started on how to test it with <code>riak_test</code>. We&rsquo;ll not go through the topic of how to write the tests itself, this might come in a later post also the <code>riak_kv</code> tests are a good point to start.</p>

<p>Please note that the approach described here is what I choose to do, it might not be best practice of the best way for you to do things. I also will link to <a href="https://github.com/Licenser/riak_test">my fork</a> of <code>riak_test</code> instead of the <a href="https://github.com/basho/riak_test">official one</a> since it includes some modifications required for testing apps other then <code>riak_kv</code> I hope those modifications will be merged back at some point but for now I want to get it ironed out a bit more before making a pull request.</p>

<h2>What is riak_test?</h2>

<p>So before we start; a few words to <code>riak_test</code>. <code>riak_test</code> is a pretty nice framework for testing distributed applications, it is, just as much as all the other <code>riak_</code> stuff created by <a href="http://basho.com">Basho</a>, pretty darn awesome.</p>

<p>At it&rsquo;s current state it is very focused on testing <code>riak_kv</code> (or <code>riak</code> as in the database) but from a first glance a lot of functionality is very universal and after all <code>riak_kv</code> is also built on top of <code>riak_core</code>, so modifying it to run with other <code>riak_core</code> based apps is pretty easy.</p>

<h2>The setup</h2>

<p>Since I will be testing multiple <code>riak_core</code> apps and not just one I decided to go the following path: Have the entire setup in a git repository, then have one branch for general fixes/changes to <code>riak_core</code> then have one branch for each application I want to test that is based on the general branch so common changes can easily be merged so it will look like this:</p>

<pre><code>---riak_test--------- (bashos master tree)
   `---riak_core----- (modifications to make riak_test work with core apps)
    ` `  `---sniffle- (tests for sniffle)
     ` `---snarl----- (tests for snarl)
      `---howl------- (tests for howl)
</code></pre>

<p>We&rsquo;ll go over this and setting up tests for the <a href="https://github.com/project-fifo/howl">howl application</a> it&rsquo;s rather small and simple and it&rsquo;s easier to follow along with something real instead of a made up situation.</p>

<h2>Getting started</h2>

<p>Step one of getting started is to get a clone from the <code>riak_test</code> repository, that&rsquo;s pretty simple (alter the path if you decided to fork):</p>

<pre><code>cd ~/Projects
git clone https://github.com/Licenser/riak_test.git
cd riak_test
</code></pre>

<p>Now we branch off to have a place to get our <code>howl</code> application but first we need to checkout the riak_core branch to make sure we get the changes included in it:</p>

<pre><code>git branch howl &amp;&amp; git checkout howl can be git checkout -b howl
</code></pre>

<p>Okay that&rsquo;s it for the basic setup not that bad so far is it?</p>

<h2>Configuration</h2>

<p>Next thing we need to do is create a configuration, at this point we&rsquo;ll assume you don&rsquo;t have one yet so we&rsquo;ll start from scratch, if you add more then one application later on you can just add them to an existing configuration.</p>

<p><code>riak_test</code> looks for the configuration file <code>~/.riak_test.config</code> and reads all the data from there so we&rsquo;ll first need to copy the sample config there:</p>

<pre><code>cp riak_test.config.sample ~/.riak_test.config
</code></pre>

<p>Next step is to open it in your favorite editor, you&rsquo;ll recognize it&rsquo;s a good old Erlang config file with tuples to group sections. We&rsquo;ll be ignoring the <code>default</code> section for now, if you&rsquo;re interested in it the documentation is quite good!</p>

<p>So lets go down to where it reads:</p>

<pre><code class="erlang">%% ===============================================================
%%  Project-specific configurations
%% ===============================================================
</code></pre>

<p>Here is where the fun starts, you&rsquo;ll see a tuple starting with <code>{rtdev,</code> - note this <code>rtdev</code> has nothing whatsoever to do with the <code>rtdev</code> that is in the <code>default</code> section as <code>{rt_harness, rtdev}</code>. The <code>rtdev</code> in the project part is just the name of the project, since your project is named <code>howl</code> not <code>rtdev</code> we&rsquo;ll go and change that first.</p>

<pre><code class="erlang">{rtdev, [
</code></pre>

<p>Now we can go to set up some variables first up the project name and executables, the name itself is just for information (or if you use <code>giddyup</code>) the executables are how your application is started, since our application is named <code>howl</code> it&rsquo;s started with the command <code>howl</code> and the admin command for it is <code>howl-admin</code>.</p>

<pre><code class="erlang">    %% The name of the project/product, used when fetching the test
    %% suite and reporting.
    {rt_project, "howl"},

    {rc_executable, "howl"},
    {rc_admin, "howl-admin"},
</code></pre>

<p>With that done come the services, those are the buggers you register in your _app.erl file, lets have a look at the howl_app.erl:</p>

<pre><code class="erlang">%...
            ok = riak_core_node_watcher:service_up(howl, self()),
%...
</code></pre>

<p>So we only have one service here we need to watch out for, named, you might guess … right <code>howl</code> that makes the list rather short:</p>

<pre><code class="erlang">    {rc_services, [howl]},
</code></pre>

<p>Now the cookie, it&rsquo;s a bit hidden in the code that you need to set it but you do, you will need this later so remember it! Since I am bad at remembering things I named it … <code>howl</code> … again.</p>

<pre><code class="erlang">    {rt_cookie, howl},
</code></pre>

<p>Now comes the setup of paths, for this we&rsquo;ve to decide where we want to put our data later on, I&rsquo;ve put all my <code>riak_test</code> things in <code>~/rt/...</code> so we&rsquo;ll follow with this. Also note that my development process works on three branches:</p>

<ul>
<li><code>test</code> - the most unstable branch.</li>
<li><code>dev</code> - here things go that should work.</li>
<li><code>master</code> - only full releases go in here.</li>
</ul>


<p>This setup might not work for you at all, but since these are only path names it should be easy enough to adapt.</p>

<p>Note that by default <code>riak_test</code> will run tests on the <code>current</code> environment</p>

<pre><code class="erlang">    %% Paths to the locations of various versions of the project. This
    %% is only valid for the `rtdev' harness.
    {rtdev_path, [
                  %% This is the root of the built `rtdev' repository,
                  %% used for manipulating the repo with git. All
                  %% versions should be inside this directory.
                  {root, "~/rt/howl"},

                  %% The path to the `current' version, which is used
                  %% exclusively except during upgrade tests.
                  {current, "~/rt/howl/howl-test"},

                  %% The path to the most immediately previous version
                  %% of the project, which is used when doing upgrade
                  %% tests.
                  {previous, "~/rt/howl/howl-dev"},

                  %% The path to the version before `previous', which
                  %% is used when doing upgrade tests.
                  {legacy, "~/rt/howl/howl-stable"}
                 ]}
]}
</code></pre>

<p>And that&rsquo;s it now the config is set up and should look like this:</p>

<pre><code class="erlang">{rtdev, [
    %% The name of the project/product, used when fetching the test
    %% suite and reporting.
    {rt_project, "howl"},

    {rc_executable, "rhowl"},
    {rc_admin, "howl-admin"},
    {rc_services, [howl]},
    {rt_cookie, howl},
    %% Paths to the locations of various versions of the project. This
    %% is only valid for the `rtdev' harness.
    {rtdev_path, [
                  %% This is the root of the built `rtdev' repository,
                  %% used for manipulating the repo with git. All
                  %% versions should be inside this directory.
                  {root, "~/rt/howl"},

                  %% The path to the `current' version, which is used
                  %% exclusively except during upgrade tests.
                  {current, "~/rt/howl/howl-test"},

                  %% The path to the most immediately previous version
                  %% of the project, which is used when doing upgrade
                  %% tests.
                  {previous, "~/rt/howl/howl-dev"},

                  %% The path to the version before `previous', which
                  %% is used when doing upgrade tests.
                  {legacy, "~/rt/howl/howl-stable"}
                 ]}
]}
</code></pre>

<h2>Setting up the application</h2>

<p>We&rsquo;ve got <code>riak_test</code> ready to test now next we need to prepare howl to be tested, we&rsquo;ll only look at the <code>current</code> (aka <code>test</code>) setup since the steps for others are pretty much the same.</p>

<p>The first step is that we need the folder, so lets create it</p>

<pre><code class="bash">mkdir -p ~/rt/raw/howl
cd ~/rt/raw/howl
</code></pre>

<p>Since howl lives with the octocat on github it&rsquo;s easy to fetch our application and checkout the test branch (remember <code>current</code> is on the test branch for me):</p>

<pre><code class="bash">git clone https://github.com/project-fifo/howl.git howl-test
cd howl-test
git checkout test
</code></pre>

<p>And done, now since it&rsquo;s a <code>riak_core</code> app we should have a task called <code>stagedevrel</code> in our makefile which will basically generate three copies of <code>howl</code> for us in the folders <code>dev/dev{1,2,3}</code> and in the process take care of compiling and getting the dependencies. I prefer <code>stagedevrel</code> over the normal <code>devrel</code> since later on it will it easier to recompile code files (<code>make</code> is enough) because it links them to the right place instead of copying them.</p>

<pre><code class="bash">make stagedevrel
</code></pre>

<p>Now we&rsquo;ve got to do a bit of cheating, <code>riak_test</code> expects the root dir to be a git repository, that is why we can&rsquo;t just put the data in there directly, so we&rsquo;ve to manually build the tree for riak core and set it up as a git repository.</p>

<pre><code class="bash">mkdir -p ~/rt/howl
cd ~/rt/howl
git init

cat &lt;&lt;EOF &gt; ~/rt/howl/.gitignore
*/dev/*/bin
*/dev/*/erts-*
*/dev/*/lib
*/dev/*/releases
*/dev/*/share
*/dev/*/snmp
EOF
git add .gitignore
</code></pre>

<p>Now we need to link our devrel files. Since howl uses [cuttlefish] I have to copy the <code>howl.config.example</code> file to <code>howl.config</code> into <code>etc</code> directory, this may be different for your setup.</p>

<pre><code class="bash">export RT_BASE=~/rt/howl/howl-test
export RC_BASE=~/rt/raw/howl/howl-test
for i in 1 2 3 4
do
  mkdir -p ${RT_BASE}/dev/dev${i}/
  cd ${RT_BASE}/dev/dev${i}/
  mkdir data etc
  touch data/.gitignore
  ln -s ${RC_BASE}/dev/dev${i}/{bin,erts-*,lib,releases,share,snmp} .
  cp ${RC_BASE}/dev/dev${i}/etc/howl.conf.example etc/howl.conf
done
</code></pre>

<p>We still need to edit the <code>vm.args</code> in <code>dev/dev{1,2,3,4}/etc/</code>  since we need to set the correct cookie - I hope you still remember yours, I told you you&rsquo;d need it (if not you can just look in the <code>~/.riak_test.config</code>)!</p>

<p>That&rsquo;s it.</p>

<p>How add the content to the git repository you initiated and commit it.</p>

<pre><code class="bash">cd $RT_BASE &amp;&amp; git add . &amp;&amp; git commit -am 'First build.'  
</code></pre>

<h2>Running a first test</h2>

<p>In the <code>riak_core</code> branch of <code>riak_test</code> I&rsquo;ve moved all the <code>riak_kv</code> specific tests from <code>tests</code> to <code>tests_riakkv</code> so you still can look at them but I left one of them in <code>tests</code>, namely the basic command test - it will check if your applications command (<code>howl</code> in our case) is well behaved.</p>

<p>We&rsquo;ll want to run it to see if <code>howl</code> is a good boy and does well to do so we&rsquo;ll need to get back into the <code>riak_test</code> folder and run the <code>riak_test</code> command:</p>

<pre><code class="bash">cd ~/Projects/riak_test
./riak_test -t tests/* -c howl -v -b none
</code></pre>

<p>I&rsquo;d like to explain this a bit, the arguments have the following meaning:</p>

<ul>
<li><code>-t tests/*</code> - we&rsquo;ll be running all tests in the folder <code>tests/</code>.</li>
<li><code>-c howl</code> - our application we want to test is named howl, this is the first element of the tuple we put in our config file when you remember.</li>
<li><code>-v</code> - This just turns on verbose output.</li>
<li><code>-b none</code> - This is still a relict from the <code>riak_kv</code> roots, it means which backend to test with, since we don&rsquo;t have backends at all we&rsquo;ll just pass none which means riak_test will happily ignore it.</li>
</ul>


<p>That&rsquo;s it! Now go and test all the things!</p>

<p>This is the first part of a series that goes on <a href="http://blog.licenser.net/blog/2013/04/09/writing-your-first-riak-test-test-yes-i-know-there-are-two-tests-there/">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Plugins With Erlang]]></title>
    <link href="http://blog.licenser.net/blog/2013/03/19/plugins-with-erlang/"/>
    <updated>2013-03-19T16:45:00+01:00</updated>
    <id>http://blog.licenser.net/blog/2013/03/19/plugins-with-erlang</id>
    <content type="html"><![CDATA[<h2>Preamble</h2>

<p>Lets start with this, Erlang releases are super useful they are one of the features I like most about Erlang - you get out an entirely self contained package you can deploy and forget, no library trouble, no wrong version of the VM no trouble at all.</p>

<p>BUT (this had to come didn&rsquo;t it) sometimes they are limiting and kind of inflexible, adding a tiny little feature means rolling out a new release, with automated builds that is not so bad but &lsquo;not so bad&rsquo; isn&rsquo;t good either. And things get worst when there are different wishes.</p>

<p>A little glimpse into reality: I&rsquo;m currently working a lot on <a href="http://project-fifo.net">Project FiFo</a> and one of the issues I faced is that - surprisingly - not everyone wants things to work exactly as I do. Which was a real shock, how could anyone ever disagree with me? Well &hellip; I got over it, really I did, still solving this issue by adding one code path for every preference and making it configureable didn&rsquo;t looked like a good solution.</p>

<p>Also recently we are thinking a lot about performance metrics, and there are like a gazillion of them and if you pick two random people I think they want three different sets of metrics. Ask again after 5 minutes and their opinions changed to 7 new metrics and certainly not the old ones!</p>

<h2>Plugins</h2>

<p>The problem is a very old one, extending the software after it was shipped, possibly letting the community extend it beyond what was dreamed of in the beginning. The solution is pretty much one day younger then the problem: plugins. Meaning a way to load code into the system.</p>

<p>Sounds easy but it is a bit more complex, just having something load into the VM doesn&rsquo;t do much good when it does not get executed in the proper place - so sadly this comes with extra work for the developer to sprinkle their code with hooks and callbacks for the plugins.</p>

<p>With this I&rsquo;d like to introduce <a href="https://github.com/Licenser/eplugin">eplugin</a> it&rsquo;s a very simplistic library for exactly that task - introduce plugins in an Erlang release or application. It takes care of discovering and loading plugins, letting them register into certain calls, a little dependency management on startup and provides functions to call registered plugins. Erlang comes with great tools already to the whole thing sums up to under 400 LOC.</p>

<h2>Types of plugins</h2>

<p>I feel it&rsquo;s kind of interesting to look at the different kind of plugins that exist and how to handle the cases with <a href="https://github.com/Licenser/eplugin">eplugin</a> also a post entirely without code would look boring.</p>

<h3>informative plugins</h3>

<p>Sometimes a plugin just want to know that something happened but don&rsquo;t care about the result. <code>eplugin</code> provides the <code>call</code> (and <code>apply</code>) functions for that. A logger is a good example for this so lets have a look:</p>

<pre><code class="erlang">%%% plugin.conf
{syslog_plugin,
  [{syslog_plugin, [{'some:event', log}]}],
  []}.

%%% syslog_plugin.erl
-module(syslog_plugin).
-export([log/1]).

log(String) -&gt;
  os:cmd("logger '" ++ String ++ "'").

%%% in your code
  %%... 
  eplugin:call('some:event', "logging this!"),
  %%... 
</code></pre>

<p>Thats pretty much it, provided you&rsquo;ve started the <code>eplugin</code> application in your code and put the plugins in the right place this will just work. You could also use this to trigger side effects, like delete all files when an error occurs to remove traces of your failure.</p>

<h3>messing around plugins</h3>

<p>This kind of plugins process some data and return a new version of this data, we have <code>fold</code> for this case. Fold since it internally uses fold to pass the data from one plugin to another. there are many applications for that one would be to replace all occurrences of &lsquo;not js&rsquo; with &lsquo;node.js&rsquo; to prevent freudian typos in your texts.</p>

<pre><code class="erlang">%%% plugin.conf
{not_js,
  [{not_js, [{'text:check', replace}]}],
  []}.

%%% not_js.erl
-module(not_js).
-export([replace/1]).

replace(String) -&gt;
  re:replace(String, "not js", "node.js", [global]).
%%% in your code
  %%... 
  String1 = eplugin:fold('text:check', "I'm writing a not js application!"),
  %%... 
</code></pre>

<p><code>fold</code> and <code>call</code> are the most interesting and important kind of plugins, they cover most if not all of the possible use cases of plugins so there is a special case left which I found useful to have.</p>

<h3>checking plugins</h3>

<p>Checking plugins are plugins which are supposed to decide if something is Ok or not, they are pretty much a case of <code>fold</code> that returns true or false (or actually whatever is not true). But <code>eplugin</code> solves this too, with the <code>test</code> function! An example here is authentication</p>

<pre><code class="erlang">%%% plugin.conf
{get_out,
  [{get_out, [{'login:allowed', no_really_not}]}],
  []}.

%%% get_out.erl
-module(get_out).
-export([no_really_not/1]).

no_really_not(Login) -&gt;
  {forbidden, ["Dear ", Login, " we don't want you here go away!"]}.

%%% in your code
  %%... 
  case eplugin:test('login:allowed', "Licenser") of
     true -&gt;
        %%% huzza!
     Error -&gt;
        io:format("~p~n", [Error])
  end
  %%... 
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Vow to Create Tickets]]></title>
    <link href="http://blog.licenser.net/blog/2013/01/31/a-vow-to-create-tickets/"/>
    <updated>2013-01-31T01:30:00+01:00</updated>
    <id>http://blog.licenser.net/blog/2013/01/31/a-vow-to-create-tickets</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve had some open source projects before, actually quite some, but Project FiFo is by far the most successful one. And aside from the technical perspective I&rsquo;ve learned a tremendous amount of new things already, one of which I want so share since. It sounds simple but I never looked at it this way before: Tickets.</p>

<p>The project has caught on momentum so fast that at &lsquo;rush hours&rsquo; people come with questions, bug reports and feature requests at rate that it&rsquo;s they pile up faster then we can help or answer, the channel and community already does a great job &lsquo;filtering&rsquo; out easy to answer topics but enough are complicated to a point where a developer has to look at them.</p>

<p>I&rsquo;ve been on both sides of the fence, and I had the honest believe that it&rsquo;s easier for developers if you just pop by the IRC channel and ask/report a issue. And that is not entirely wrong often a quick &lsquo;hey I&rsquo;ve problem X/Y&rsquo; is enough to solve a situation others had before that can be resoved with a few words.</p>

<p>What I did not realise is how crucial it is to open tickets for anything that can&rsquo;t be resolved directly. The reason for this is simple, usually there are N users for 1 developer and for each user it&rsquo;s easy to keep their topic in mind and present it can become very had very fast for a developer with three users to listen to.</p>

<p>But I don&rsquo;t want to wander of ranting, instead I&rsquo;ll try to explain why a ticket really helps me to get stuff done.</p>

<h4>Organisation</h4>

<p>Tickets make it very easy to keep track of them, assign them to people (given you&rsquo;re not solving them yourself) and add followup data. Tickets can be prioritised and categorised, this makes it very easy to group related items or even look up known issues.</p>

<p>The more tickets there are and the more people working on them the more important this gets, handling six tickets gets way easier then handling six email conversations or unthreading six conversations from a IRC channel.</p>

<h4>Information</h4>

<p>One of the great things about tickets is that they can contain additional information, it&rsquo;s absolutely helpful to look at a issue that already has some information attached that goes further then three lines and a &ldquo;it isn&rsquo;t working&rdquo;.</p>

<p>To make this better it is also possible to add files and logs which can contain much more information to a ticket and those can be &lsquo;handed around&rsquo; between different people looking at the issue without the need to send mails.</p>

<h4>Followup</h4>

<p>Tickets are a two way street, once a ticket is logged the reporter can see it&rsquo;s progress without having to ask - or at least can ask if it is not handled for a long period. Also it is easier to say &lsquo;hey what is the status if ticket #42&rsquo; where all the information is provided already instead of &lsquo;hey how about that bug that happened when …&rsquo; and have to explain it all over again.</p>

<h4>Deduplication</h4>

<p>Once a ticket is logged it&rsquo;s visible for everyone, there is no reason to go around and ask &lsquo;hey do you know the bug …&rsquo; or &lsquo;what are your thoughts about feature …&rsquo; and perhaps even get a wrong answer since you&rsquo;re talking to more then one person. This makes everyones live easier.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Erlang and More DTrace]]></title>
    <link href="http://blog.licenser.net/blog/2013/01/28/erlang-and-more-dtrace/"/>
    <updated>2013-01-28T18:33:00+01:00</updated>
    <id>http://blog.licenser.net/blog/2013/01/28/erlang-and-more-dtrace</id>
    <content type="html"><![CDATA[<p>Some nice additions to the little Erlang DTrace demo. For once I&rsquo;ve added a filed to input custom scripts which are run on the server which is pretty neat since it allows running all kind of (l)quantize based scripts from the server and get a nice heatmap.</p>

<p>Like how about the heatmap of how Erlang function call times?</p>

<p><img src="http://blog.licenser.net/images/posts/2013-01-28-erlang-and-more-dtrace-1.png" alt="Erlang Call Heatmap" /></p>

<p>The script used (also included in the repository):
<code>d
erlang*:::global-function-entry
{
  self-&gt;funcall_entry_ts[copyinstr(arg1)] = vtimestamp;
}
erlang*:::function-return
{
  @time[copyinstr(arg1)] = lquantize((vtimestamp - self-&gt;funcall_entry_ts[copyinstr(arg1)] ) / 1000, 0, 63, 2);
}
</code></p>

<p>Now that is already cool <strong>but</strong> there is more, in addition to there is a page now that allows to show list based queries (as count or sum) so for example it would be very easy to get a profiling of an Erlang program like this:</p>

<p><img src="http://blog.licenser.net/images/posts/2013-01-28-erlang-and-more-dtrace-2.png" alt="Erlang Profiling" /></p>

<p>The script used (also included in the repository):
<code>d
erlang*:::global-function-entry
{
  self-&gt;funcall_entry_ts[copyinstr(arg1)] = vtimestamp;
}
erlang*:::function-return
{
  @time[copyinstr(arg1)] = sum((vtimestamp - self-&gt;funcall_entry_ts[copyinstr(arg1)] ) / 1000);
}
</code></p>

<p>Cool thing is this profiling can be turned on and off in a live system and has a <strong>comparable low</strong> performance impact (less then 50% unless functions are hammering in my tests).</p>

<p>To add to the joy, the scripts are stopped the moment the page is closed eliminating every kind of overhead, without restarting anything!</p>

<p>So lets sum this up, the old news (when you played with dtrace before) is that you can profile and analyse your applications on the fly with minimal impact, but the funky part is that you can do it directly as part of your application and only shows when you actually look at the page :) it&rsquo;s kind of like quantumanalytics just the other way round!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Erlang and DTrace]]></title>
    <link href="http://blog.licenser.net/blog/2013/01/25/erlang-and-dtrace/"/>
    <updated>2013-01-25T00:21:00+01:00</updated>
    <id>http://blog.licenser.net/blog/2013/01/25/erlang-and-dtrace</id>
    <content type="html"><![CDATA[<p>As part of <a href="http://project-fifo.net">Project FiFo</a> I&rsquo;ve invested some time to research into DTrace and Erlang, not the probes that are there since some time but a DTrace consumer - letting you execute DTrace scripts from within Erlang and read the results.</p>

<p>The result of this research is the <a href="https://www.github.com/project-fifo/erltrace">erltrace</a> a consumer for DTrace implemented as a Erlang NIF. The NIF is based on the <a href="https://github.com/bcantrill/node-libdtrace">node.js</a> and and <a href="http://tmetsch.github.com/python-dtrace/">python</a> implementations - many thanks to them!</p>

<p>It&rsquo;s pretty easy to consume dtrace data from within erlang with <a href="https://www.github.com/project-fifo/erltrace">erltrace</a> and I figured lets make a little demo. Since I&rsquo;m not a big fan of reinventing the wheel and a simple demo had been done before, namely <a href="http://howtonode.org/heat-tracer">heat-tracer</a>.</p>

<p>It&rsquo;s a nice idea and simple enough to implement and cowboy gives a very nice base for that in Erlang. So there you go <a href="https://github.com/Licenser/dowboy">dowboy</a>. The HTML/JS part of the page is pretty much the same as in the original save for replacing socket.io with simple websockets, I&rsquo;ll skip this and the the cowboy parts to look directly in the interesting parts.</p>

<p>When connecting we set up a timer to inform us every second to gather the results from DTrace:</p>

<pre><code class="erlang">websocket_init(_Any, Req, []) -&gt;
    timer:send_interval(1000, tick),
    Req2 = cowboy_http_req:compact(Req),
    {ok, Req2, undefined, hibernate}.
</code></pre>

<p>Next up we handle incoming Websockets messages, this deals very simple. Every string that is send is considered a new dtrace script to execute.</p>

<p>The first part creates a new handler, this is pretty much a reference to the libdtrace internal data structures. Since we allow multiple scripts to be send we also have to ensure that the old sockets are closed before new ones are opened.</p>

<pre><code class="erlang">websocket_handle({text, Msg}, Req, State) -&gt;
    %% We create a new handler
    {ok, Handle} = case State of
                       undefined -&gt;
                           erltrace:open();
                       {Old} -&gt;
                           %% But we want to make sure that any old one is closed first.
                           erltrace:close(Old),
                           erltrace:open()
                   end,
</code></pre>

<p>Next we compile the dtrace script, <a href="https://www.github.com/project-fifo/erltrace">erltrace</a> only takes lists as strings and not binaries so we&rsquo;ve to convert it first and then we pass it along with our handle to get the script compiled. It will return <code>ok</code> if everything went well.</p>

<pre><code class="erlang">    %% We've to confert cowboys binary to a list.
    Msg1 = binary_to_list(Msg),
    ok = erltrace:compile(Handle, Msg1),
</code></pre>

<p>Okay now that we&rsquo;ve compiled the script we just need to tell dtrace to start running it, this happens with the <code>erltrace:go</code> call. Again it will return <code>ok</code> when everything is fine. Finally we just output the script as debug info and return.</p>

<pre><code class="erlang">    ok = erltrace:go(Handle),
    io:format("SCRIPT&gt; ~s~n", [Msg]),
    {ok, Req, {Msg1, Handle}};
</code></pre>

<p>Next up: reading the dtrace data, <code>erltrace:walk</code> do that for you and hand back a datastructur to parse. It is returned as <code>{ok, Data}</code> when there is something to handle.</p>

<pre><code class="erlang">websocket_info(tick, Req, {Msg, Handle} = State) -&gt;
     case erltrace:walk(Handle) of
         {ok, R} -&gt;
</code></pre>

<p>Now since we have JSON on the other side we need to transform the data here Erlangs list comprehensions come to the rescue. Data is returned as <code>[{lquantize, [Name], { {BucketStart, BucketEnd
}, BucketCount}}]</code> and we want it in the form <code>{Name, [[BucketStart, BucketEnd], BucketCount]</code> so here you go. We can tehn simpley encode this with <a href="https://github.com/talentdeficit/jsx">jsx</a> and send it over the wire:</p>

<pre><code class="erlang">             JSON = [{list_to_binary(Call),[ [[S, E], V]|| { {S, E}, V} &lt;- Vs]}|| {lquantize, [Call], Vs} &lt;- R],
             {reply, {text, jsx:encode(JSON)}, Req, State, hibernate};
</code></pre>

<p><code>ok</code> will be returned if there is no data yet to consume, we simply do nothing here.</p>

<pre><code class="erlang">         ok -&gt;
             {ok, Req, {Msg, Handle1}};
</code></pre>

<p>The last case is just for making things proper, if an error is returned we close stop the current handle and create a new one the same we did in the init.</p>

<pre><code class="erlang">         Other -&gt;
             io:format("Error: ~p", [E]),
             try
                 erltrace:stop(Handle)
             catch
                 _:_ -&gt;
                     ok
             end,
             {ok, Handle1} = erltrace:open(),
             erltrace:compile(Handle1, Msg),
             erltrace:go(Handle),
             {ok, Req, {Msg, Handle1}}
     end;
</code></pre>

<p>Now that all put together and run it on a dtrace capable machine we get a nice little heatmap that updates every second:</p>

<p><img src="http://blog.licenser.net/images/posts/2013-01-25-erlang-and-dtrace-heatmap.png" alt="Heatmap" /></p>

<p>Neat isn&rsquo;t it?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A New Start]]></title>
    <link href="http://blog.licenser.net/blog/2013/01/24/a-new-start/"/>
    <updated>2013-01-24T22:35:00+01:00</updated>
    <id>http://blog.licenser.net/blog/2013/01/24/a-new-start</id>
    <content type="html"><![CDATA[<p>Now it has been a while, it&rsquo;s surprisingly hard to find a decent way to blog in the end I landed with <a href="http://octropress.org">Octopress</a>. Lets see how that turns out, so far after a slightly bumpy bumpy start it seems decent enough.</p>
]]></content>
  </entry>
  
</feed>
